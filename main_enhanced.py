#!/usr/bin/env python3
"""
å¯Œé€”MCPæœåŠ¡å¢å¼ºç‰ˆ - é›†æˆç¼“å­˜å’ŒæŠ€æœ¯åˆ†æåŠŸèƒ½
æ”¯æŒæ™ºèƒ½ç¼“å­˜ã€æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€å½¢æ€è¯†åˆ«ç­‰é«˜çº§åŠŸèƒ½
"""

import asyncio
import time
import secrets
import json
import re
from pathlib import Path
from datetime import datetime, timedelta, timezone, time as dtime
from email.utils import parsedate_to_datetime
from typing import Dict, List, Optional, Any, Tuple
from pydantic import BaseModel, Field
from collections import defaultdict
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger as log  # Use alias to avoid conflicts
from contextlib import asynccontextmanager
from futu import *
from fastapi import Request
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles

# Ensure we use loguru logger after futu import
logger = log

# å¯¼å…¥åŸæœ‰æ¨¡å—
from services.futu_service import FutuService
from models.futu_models import *
from models.analysis_models import *
from config import settings

# å¯¼å…¥æ–°åŠŸèƒ½æ¨¡å—  
from cache.cache_manager import DataCacheManager, CacheConfig
from analysis.technical_indicators import TechnicalIndicators, TechnicalData, IndicatorConfig
# å¯¼å…¥åŸºæœ¬é¢æœç´¢æœåŠ¡
from services.fundamental_service import fundamental_service
from models.fundamental_models import FundamentalSearchRequest, FundamentalSearchResponse
from services.recommendation_storage import RecommendationStorageService
from models.recommendation_models import RecommendationWriteRequest, RecommendationQueryRequest, RecommendationUpdateRequest, RecommendationReevaluateRequest
from models.dashboard_models import DashboardSessionRequest, DashboardSessionResponse, DashboardSessionItem
from services.dashboard_stream import DashboardStreamManager
from services.dashboard_session_store import DashboardSessionStore
from services.deepseek_service import DeepSeekService
from services.fundamental_storage import FundamentalNewsStorage
from services.minute_kline_storage import MinuteKlineStorage
from services.multi_model_service import MultiModelAnalysisService
from services.strategy_monitor import StrategyMonitorService

# å…¨å±€å˜é‡
futu_service: Optional[FutuService] = None
cache_manager: Optional[DataCacheManager] = None
recommendation_storage: Optional[RecommendationStorageService] = None
deepseek_service: Optional[DeepSeekService] = None
fundamental_storage: Optional[FundamentalNewsStorage] = None
_server_ready = False
dashboard_sessions: Dict[str, Dict[str, Any]] = {}
_dashboard_lock = asyncio.Lock()
dashboard_stream_manager: Optional[DashboardStreamManager] = None
dashboard_session_store: Optional[DashboardSessionStore] = None
minute_kline_storage: Optional[MinuteKlineStorage] = None
multi_model_service: Optional[MultiModelAnalysisService] = None
strategy_monitor: Optional[StrategyMonitorService] = None
DASHBOARD_DIST_PATH = Path("web/dashboard-app/dist")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global futu_service, cache_manager, recommendation_storage, dashboard_stream_manager, dashboard_session_store, _server_ready, deepseek_service, fundamental_storage, minute_kline_storage, multi_model_service, strategy_monitor
    
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆMCP FutuæœåŠ¡...")
    
    try:
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        cache_config = CacheConfig(
            redis_url="redis://localhost:6379",
            sqlite_path="data/futu_cache.db",
            memory_max_size=2000,
            redis_expire_seconds=7200
        )
        cache_manager = DataCacheManager(cache_config)
        logger.info("âœ… ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        recommendation_storage = RecommendationStorageService(db_path="data/recommendations.db")
        logger.info("âœ… ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        dashboard_session_store = DashboardSessionStore(path="data/dashboard_sessions.json")
        logger.info("âœ… çœ‹æ¿ä¼šè¯å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        fundamental_storage = FundamentalNewsStorage(db_path="data/fundamental_news.db")
        logger.info("âœ… åŸºæœ¬é¢èµ„è®¯å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        minute_kline_storage = MinuteKlineStorage(db_path="data/minute_kline.db")
        logger.info("âœ… åˆ†é’Ÿçº§Kçº¿å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        if settings.deepseek_api_key:
            # å¦‚æœä½¿ç”¨ Speciale base_url ä¸”æœªæŒ‡å®šæ¨¡å‹ï¼Œè‡ªåŠ¨ä½¿ç”¨ special æ¨¡å‹å
            base_url = settings.deepseek_base_url or "https://api.deepseek.com/v1/chat/completions"
            is_speciale = "speciale" in (base_url or "").lower()
            default_model = "deepseek-v3.2-speciale" if is_speciale else "deepseek-v3.2"
            default_fundamental_model = default_model
            deepseek_service = DeepSeekService(
                settings.deepseek_api_key,
                base_url=base_url,
                model=settings.deepseek_model or default_model,
                fundamental_model=settings.deepseek_fundamental_model or default_fundamental_model,
            )
            logger.info("âœ… DeepSeekåˆ†ææœåŠ¡å·²å¯ç”¨")
        else:
            logger.warning("âš ï¸ DeepSeek API KEY æœªé…ç½®ï¼ŒåŸºæœ¬é¢é«˜çº§åˆ†æä¸å¯ç”¨")
        multi_model_service = MultiModelAnalysisService(deepseek_service)
        
        # åˆå§‹åŒ–å¯Œé€”æœåŠ¡
        futu_service = FutuService()
        # è®¾ç½®ç¼“å­˜ç®¡ç†å™¨
        futu_service.cache_manager = cache_manager

        # æœ¬åœ°å¯†é’¥æ£€æŸ¥ï¼ˆä¸å½±å“æœåŠ¡å¯åŠ¨ï¼Œä½†æé†’ç”¨æˆ·ï¼‰
        if not settings.metaso_api_key:
            logger.warning("âš ï¸ Metaso API å¯†é’¥æœªé…ç½®ã€‚æœ¬åœ°ä½¿ç”¨è¯·åœ¨ .env è®¾ç½® METASO_API_KEYã€‚")
        if not (settings.kimi_moonshot_key or settings.kimi_api_key):
            logger.warning("âš ï¸ Kimi API å¯†é’¥æœªé…ç½®ã€‚æœ¬åœ°ä½¿ç”¨è¯·åœ¨ .env è®¾ç½® KIMI_MOONSHOT_KEYã€‚")
        
        # å°è¯•è¿æ¥å¯Œé€”OpenD
        if await futu_service.connect():
            logger.info("âœ… å¯Œé€”OpenDè¿æ¥æˆåŠŸ")
        else:
            logger.warning("âš ï¸  å¯Œé€”OpenDè¿æ¥å¤±è´¥ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        
        if dashboard_session_store:
            loaded_sessions = await asyncio.to_thread(dashboard_session_store.load)
            async with _dashboard_lock:
                for session_id, info in loaded_sessions.items():
                    info.setdefault("history", [])
                    dashboard_sessions[session_id] = info
            logger.info(f"âœ… å·²æ¢å¤ {len(loaded_sessions)} ä¸ªçœ‹æ¿ä¼šè¯")
        
        try:
            loop = asyncio.get_running_loop()
            dashboard_stream_manager = DashboardStreamManager(futu_service, loop, minute_kline_storage)
            if dashboard_sessions:
                for session_id, info in dashboard_sessions.items():
                    code = info.get("code")
                    if code:
                        dashboard_stream_manager.register_session(session_id, code)
                unique_codes = {sess["code"] for sess in dashboard_sessions.values() if sess.get("code")}
                for code in unique_codes:
                    await dashboard_stream_manager.ensure_code_subscription(code)
            dashboard_stream_manager.start_guard()
        except RuntimeError:
            logger.warning("æœªè·å–åˆ°äº‹ä»¶å¾ªç¯ï¼ŒdashboardæµåŠŸèƒ½ä¸å¯ç”¨")
        
        strategy_monitor = StrategyMonitorService(futu_service, recommendation_storage)
        await strategy_monitor.start()

        # ç­‰å¾…æœåŠ¡å®Œå…¨åˆå§‹åŒ–
        await asyncio.sleep(3)
        
        _server_ready = True
        logger.info("âœ… Web API æœåŠ¡åˆå§‹åŒ–å®Œæˆ (MCP å·²æ‹†åˆ†ä¸ºç‹¬ç«‹è¿›ç¨‹)")
        if settings.external_mcp_endpoint:
            logger.info(f"ğŸ“¡ å¤–éƒ¨ MCP è®¿é—®åœ°å€: {settings.external_mcp_endpoint}")
            
        yield
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        raise
    finally:
        # æ¸…ç†èµ„æº
        _server_ready = False
        if futu_service:
            await futu_service.disconnect()
        if strategy_monitor:
            await strategy_monitor.stop()
        minute_kline_storage = None
        logger.info("ğŸ”¥ æœåŠ¡å·²åœæ­¢")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="å¯Œé€” MCP å¢å¼ºæœåŠ¡",
    description="é›†æˆæ™ºèƒ½ç¼“å­˜ã€æŠ€æœ¯åˆ†æã€å½¢æ€è¯†åˆ«ç­‰åŠŸèƒ½çš„ä¸“ä¸šè‚¡ç¥¨åˆ†æå¹³å°",
    version="2.0.0",
    lifespan=lifespan
)

if DASHBOARD_DIST_PATH.exists():
    app.mount("/assets", StaticFiles(directory=DASHBOARD_DIST_PATH / "assets"), name="dashboard_assets")
    if (DASHBOARD_DIST_PATH / "vite.svg").exists():
        app.mount("/vite.svg", StaticFiles(directory=DASHBOARD_DIST_PATH), name="dashboard_vite")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/web/dashboard", include_in_schema=False)
async def dashboard_page():
    """è¿”å›å‰ç«¯ä»ªè¡¨æ¿é¡µé¢"""
    if DASHBOARD_DIST_PATH.exists():
        dist_index = DASHBOARD_DIST_PATH / "index.html"
        if dist_index.exists():
            return FileResponse(dist_index)
    file_path = Path("web/dashboard.html")
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="dashboard page not found")
    return FileResponse(file_path)


@app.get("/web", include_in_schema=False)
async def dashboard_home():
    """å¯è§†åŒ–é¦–é¡µ"""
    if DASHBOARD_DIST_PATH.exists():
        dist_index = DASHBOARD_DIST_PATH / "index.html"
        if dist_index.exists():
            return FileResponse(dist_index)
    file_path = Path("web/index.html")
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="home page not found")
    return FileResponse(file_path)

# ==================== å¯åŠ¨äº‹ä»¶å¤„ç† ====================
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶ - ç¡®ä¿MCPå®Œå…¨åˆå§‹åŒ–"""
    global _server_ready
    
    # ç­‰å¾…é¢å¤–çš„åˆå§‹åŒ–æ—¶é—´
    await asyncio.sleep(2)
    
    if not _server_ready:
        logger.warning("âš ï¸  æœåŠ¡å™¨åˆå§‹åŒ–å»¶è¿Ÿï¼Œè¯·ç¨åé‡è¯•è¿æ¥")
    else:
        logger.info("âœ… Web API æœåŠ¡å·²å°±ç»ª")


# ==================== å¥åº·æ£€æŸ¥ ====================
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    cache_stats = await cache_manager.get_cache_stats() if cache_manager else {}
    
    return {
        "status": "healthy" if _server_ready else "degraded",
        "futu_connected": _server_ready,
        "cache_available": cache_manager is not None,
        "mcp_proxy": {
            "mode": "external",
            "endpoint": settings.external_mcp_endpoint
        },
        "metaso_configured": settings.metaso_api_key is not None,
        "kimi_configured": settings.kimi_api_key is not None,
        "timestamp": datetime.now().isoformat(),
        "cache_stats": cache_stats
    }

@app.get("/mcp/status")
async def mcp_status():
    """MCPçŠ¶æ€æ£€æŸ¥ï¼šè®©å®¢æˆ·ç«¯åœ¨è¿æ¥å‰ç¡®è®¤å°±ç»ª"""
    ready = settings.external_mcp_endpoint is not None
    return {
        "mcp_ready": ready,
        "server_ready": _server_ready,
        "mode": "external",
        "external_endpoint": settings.external_mcp_endpoint,
        "can_accept_connections": ready,
        "timestamp": datetime.now().isoformat(),
        "message": "MCPæœåŠ¡å·²æ‹†åˆ†ä¸ºç‹¬ç«‹è¿›ç¨‹ï¼Œè¯·ç›´æ¥è¿æ¥ external_endpoint"
    }


@app.api_route("/mcp{extra_path:path}", methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"])
async def mcp_redirect(extra_path: str, request: Request):
    """å°†æ—§çš„ /mcp è¯·æ±‚é‡å®šå‘åˆ°ç‹¬ç«‹çš„ MCP æœåŠ¡"""
    if not settings.external_mcp_endpoint:
        raise HTTPException(status_code=503, detail="MCPæœåŠ¡æœªé…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
    suffix = extra_path or ""
    if suffix and not suffix.startswith("/"):
        suffix = "/" + suffix
    target = settings.external_mcp_endpoint.rstrip("/") + suffix
    return RedirectResponse(url=target, status_code=307)


# ==================== ç»¼åˆåˆ†ææ¥å£ ====================
@app.post("/api/analysis/snapshot",
          operation_id="get_analysis_snapshot",
          summary="è·å–è‚¡ç¥¨ç»¼åˆåˆ†æå¿«ç…§",
          tags=["analysis"])
async def get_analysis_snapshot(request: AnalysisSnapshotRequest) -> Dict[str, Any]:
    """è¿”å›å•æ”¯è‚¡ç¥¨çš„å…¨é‡åˆ†æå¿«ç…§ï¼Œç”¨äº MCP/Agent å¿«é€Ÿè¯»å–"""
    code = request.code
    logger.info(f"[analysis.snapshot] code={code} start")
    start = time.time()

    def _noop(result=None):
        return asyncio.create_task(asyncio.sleep(0, result=result))

    quote_task = asyncio.create_task(_fetch_quote_snapshot(code))
    signals_task = asyncio.create_task(_fetch_news_signals(code, 12)) if request.include_signals else _noop({})
    rec_task = asyncio.create_task(_fetch_recommendations_snapshot(code)) if request.include_recommendations else _noop([])
    holding_task = asyncio.create_task(_fetch_holding_snapshot(code)) if request.include_holding else _noop(None)

    if request.include_history and futu_service:
        history_task = asyncio.create_task(
            futu_service.get_history_kline(
                HistoryKLineRequest(
                    code=code,
                    ktype=request.history_ktype,
                    max_count=request.history_points
                )
            )
        )
    else:
        history_task = _noop()

    if request.include_capital_flow and futu_service:
        flow_task = asyncio.create_task(
            futu_service.get_capital_flow(
                CapitalFlowRequest(code=code, period_type=PeriodType.DAY)
            )
        )
    else:
        flow_task = _noop()

    if request.include_capital_distribution and futu_service:
        distribution_task = asyncio.create_task(
            futu_service.get_capital_distribution(CapitalDistributionRequest(code=code))
        )
    else:
        distribution_task = _noop()

    if request.include_technicals:
        technical_req = TechnicalAnalysisRequest(
            code=code,
            period=request.technical_period,
            indicators=request.technical_indicators
        )
        technical_task = asyncio.create_task(get_technical_indicators(technical_req))
    else:
        technical_task = _noop()

    (quote, signals, recommendations, holding,
     history_resp, flow_resp, distribution_resp, technical_resp) = await asyncio.gather(
        quote_task, signals_task, rec_task, holding_task,
        history_task, flow_task, distribution_task, technical_task
    )

    snapshot: Dict[str, Any] = {
        "code": code,
        "generated_at": datetime.now().isoformat(),
        "quote": quote,
        "session_window": _get_session_window(code)
    }

    if request.include_signals and signals:
        snapshot["signals"] = signals
    if request.include_recommendations and recommendations:
        snapshot["recommendations"] = recommendations
    if request.include_holding and holding:
        snapshot["holding"] = holding
    if request.include_history and isinstance(history_resp, APIResponse) and history_resp.ret_code == 0:
        snapshot["history_kline"] = history_resp.data.get("kline_data")
    if request.include_capital_flow and isinstance(flow_resp, APIResponse) and flow_resp.ret_code == 0:
        snapshot["capital_flow"] = flow_resp.data
    if request.include_capital_distribution and isinstance(distribution_resp, APIResponse) and distribution_resp.ret_code == 0:
        snapshot["capital_distribution"] = distribution_resp.data
    if request.include_technicals and isinstance(technical_resp, dict) and technical_resp.get("ret_code") == 0:
        snapshot["technical_indicators"] = technical_resp.get("data")
        summary = technical_resp.get("data", {}).get("summary") if technical_resp.get("data") else None
        if summary:
            snapshot.setdefault("insights", {})["technical_summary"] = summary

    if signals:
        bullish = len(signals.get("bullish", []))
        bearish = len(signals.get("bearish", []))
        neutral = len(signals.get("neutral", []))
        snapshot.setdefault("insights", {}).update({
            "signal_bullish": bullish,
            "signal_bearish": bearish,
            "signal_neutral": neutral
        })
    if recommendations:
        snapshot.setdefault("insights", {})["recommendation_count"] = len(recommendations)
    if quote:
        snapshot.setdefault("insights", {})["last_price"] = quote.get("price") or quote.get("cur_price")

    cost = time.time() - start
    snapshot["execution_time"] = cost
    logger.info(f"[analysis.snapshot.done] code={code} cost={cost:.3f}s")
    return snapshot


class MultiModelAnalysisRequest(BaseModel):
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç , å¦‚ HK.00700")
    models: List[str] = Field(default_factory=lambda: ["deepseek", "kimi"], description="å‚ä¸åˆ†æçš„æ¨¡å‹åˆ—è¡¨")
    judge_model: str = Field("gemini", description="æœ€ç»ˆè¯„å®¡æ¨¡å‹")
    question: Optional[str] = Field(None, description="é¢å¤–å…³æ³¨çš„é—®é¢˜")


class SingleModelAnalysisRequest(BaseModel):
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç , å¦‚ HK.00700")
    model: str = Field(..., description="éœ€è¦è°ƒç”¨çš„æ¨¡å‹: deepseek/kimi/gemini")
    question: Optional[str] = Field(None, description="é¢å¤–é—®é¢˜")


class MultiModelJudgeRequest(BaseModel):
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç , å¦‚ HK.00700")
    judge_model: str = Field("gemini", description="è¯„å®¡æ¨¡å‹, ä¾‹å¦‚ gemini/deepseek")
    base_results: List[Dict[str, Any]] = Field(..., description="å„æ¨¡å‹çš„åŸå§‹ç»“æœ")
    question: Optional[str] = Field(None, description="é¢å¤–é—®é¢˜")


class FundamentalNewsRefreshRequest(BaseModel):
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç , å¦‚ HK.00700")
    size: int = Field(10, ge=10, le=100, description="Metaso æœç´¢æ•°é‡")
    days: int = Field(3, ge=1, le=30, description="é™å®šæœ€è¿‘Nå¤©çš„èµ„è®¯ï¼Œé»˜è®¤3å¤©")


@app.post("/api/analysis/multi_model",
          operation_id="run_multi_model_analysis",
          summary="å¤šæ¨¡å‹å¹¶è¡Œç­–ç•¥åˆ†æ",
          tags=["analysis"])
async def run_multi_model_analysis(request: MultiModelAnalysisRequest) -> Dict[str, Any]:
    if not _server_ready:
        raise HTTPException(status_code=503, detail="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­")
    if not multi_model_service:
        raise HTTPException(status_code=503, detail="å¤šæ¨¡å‹åˆ†ææœåŠ¡ä¸å¯ç”¨")
    snapshot = await get_analysis_snapshot(AnalysisSnapshotRequest(code=request.code))
    context_text = _build_analysis_context_text(snapshot)
    result = await multi_model_service.run_analysis(
        code=request.code,
        models=request.models,
        judge_model=request.judge_model,
        context_text=context_text,
        context_snapshot=snapshot,
        question=request.question,
    )
    return result


@app.post(
    "/api/analysis/multi_model/model",
    operation_id="run_single_model_analysis",
    summary="å•æ¨¡å‹ç­–ç•¥åˆ†æ",
    tags=["analysis"],
)
async def run_single_model_analysis(request: SingleModelAnalysisRequest) -> Dict[str, Any]:
    if not _server_ready or not multi_model_service:
        raise HTTPException(status_code=503, detail="å¤šæ¨¡å‹åˆ†ææœåŠ¡ä¸å¯ç”¨")
    snapshot = await get_analysis_snapshot(AnalysisSnapshotRequest(code=request.code))
    context_text = _build_analysis_context_text(snapshot)
    context_snapshot = dict(snapshot)
    context_snapshot["context_text"] = context_text
    result = await multi_model_service.run_single_analysis(
        code=request.code,
        model=request.model,
        context_text=context_text,
        question=request.question,
    )
    return {**result, "context_snapshot": context_snapshot}


@app.post(
    "/api/analysis/multi_model/judge",
    operation_id="run_multi_model_judge",
    summary="å¤šæ¨¡å‹è¯„å®¡æ•´åˆ",
    tags=["analysis"],
)
async def run_multi_model_judge(request: MultiModelJudgeRequest) -> Dict[str, Any]:
    if not _server_ready or not multi_model_service:
        raise HTTPException(status_code=503, detail="å¤šæ¨¡å‹åˆ†ææœåŠ¡ä¸å¯ç”¨")
    snapshot = await get_analysis_snapshot(AnalysisSnapshotRequest(code=request.code))
    context_text = _build_analysis_context_text(snapshot)
    context_snapshot = dict(snapshot)
    context_snapshot["context_text"] = context_text
    judge = await multi_model_service.run_judge_only(
        code=request.code,
        judge_model=request.judge_model,
        context_text=context_text,
        base_results=request.base_results,
        question=request.question,
    )
    judge["context_snapshot"] = context_snapshot
    return judge


# ==================== æ—¶é—´ç›¸å…³æ¥å£ ====================
@app.get("/api/time/current",
         operation_id="get_current_time",
         summary="è·å–å½“å‰æ—¶é—´",
         description="è·å–æœåŠ¡å™¨å½“å‰æ—¶é—´ï¼Œç”¨äºLLMç†è§£æ—¶é—´ä¸Šä¸‹æ–‡å’Œæ¨¡ç³Šæ—¶é—´è¡¨è¾¾")
async def get_current_time() -> Dict[str, Any]:
    """è·å–å½“å‰æ—¶é—´ä¿¡æ¯ï¼Œå¸®åŠ©LLMç†è§£æ¨¡ç³Šæ—¶é—´è¡¨è¾¾"""
    now = datetime.now()
    
    # è®¡ç®—ä¸€äº›å¸¸ç”¨çš„æ—¶é—´ç‚¹
    today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
    yesterday_start = today_start - timedelta(days=1)
    week_start = today_start - timedelta(days=now.weekday())  # æœ¬å‘¨ä¸€
    month_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    
    # è‚¡å¸‚ç›¸å…³æ—¶é—´ï¼ˆæ¸¯è‚¡ä¸ºä¾‹ï¼‰
    market_open_today = now.replace(hour=9, minute=30, second=0, microsecond=0)
    market_close_today = now.replace(hour=16, minute=0, second=0, microsecond=0)
    
    # åˆ¤æ–­æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´
    is_trading_hours = False
    if now.weekday() < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
        morning_session = (now.replace(hour=9, minute=30) <= now <= now.replace(hour=12, minute=0))
        afternoon_session = (now.replace(hour=13, minute=0) <= now <= now.replace(hour=16, minute=0))
        is_trading_hours = morning_session or afternoon_session
    
    # ç”Ÿæˆæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯
    time_contexts = {
        "ä»Šå¤©": today_start.strftime("%Y-%m-%d"),
        "æ˜¨å¤©": yesterday_start.strftime("%Y-%m-%d"),
        "æœ¬å‘¨": week_start.strftime("%Y-%m-%d"),
        "æœ¬æœˆ": month_start.strftime("%Y-%m-%d"),
        "è¿‘æœŸ": (now - timedelta(days=7)).strftime("%Y-%m-%d"),  # æœ€è¿‘7å¤©
        "æœ€è¿‘": (now - timedelta(days=3)).strftime("%Y-%m-%d"),  # æœ€è¿‘3å¤©
        "è¿™å‡ å¤©": (now - timedelta(days=5)).strftime("%Y-%m-%d"),  # æœ€è¿‘5å¤©
        "ä¸Šå‘¨": (week_start - timedelta(days=7)).strftime("%Y-%m-%d"),
        "ä¸Šæœˆ": (month_start - timedelta(days=1)).replace(day=1).strftime("%Y-%m-%d"),
        "æœ€è¿‘ä¸€ä¸ªæœˆ": (now - timedelta(days=30)).strftime("%Y-%m-%d"),
        "æœ€è¿‘ä¸‰ä¸ªæœˆ": (now - timedelta(days=90)).strftime("%Y-%m-%d")
    }
    
    return {
        "ret_code": 0,
        "ret_msg": "è·å–å½“å‰æ—¶é—´æˆåŠŸ",
        "data": {
            # åŸºç¡€æ—¶é—´ä¿¡æ¯
            "current_time": now.isoformat(),
            "current_date": now.strftime("%Y-%m-%d"),
            "current_datetime": now.strftime("%Y-%m-%d %H:%M:%S"),
            "timestamp": int(now.timestamp()),
            
            # æ ¼å¼åŒ–æ—¶é—´
            "formatted": {
                "iso": now.isoformat(),
                "chinese": now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S"),
                "date_only": now.strftime("%Y-%m-%d"),
                "time_only": now.strftime("%H:%M:%S"),
                "weekday": now.strftime("%A"),
                "weekday_chinese": ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
            },
            
            # å¸‚åœºæ—¶é—´ä¿¡æ¯
            "market": {
                "is_trading_day": now.weekday() < 5,
                "is_trading_hours": is_trading_hours,
                "market_open_time": market_open_today.strftime("%H:%M"),
                "market_close_time": market_close_today.strftime("%H:%M"),
                "next_trading_day": _get_next_trading_day(now).strftime("%Y-%m-%d")
            },
            
            # æ—¶é—´ä¸Šä¸‹æ–‡æ˜ å°„ï¼ˆç”¨äºæ¨¡ç³Šæ—¶é—´ç†è§£ï¼‰
            "time_contexts": time_contexts,
            
            # æ—¶é—´åŒºé—´å»ºè®®
            "common_periods": {
                "æœ€è¿‘1å¤©": {
                    "start": (now - timedelta(days=1)).strftime("%Y-%m-%d"),
                    "end": now.strftime("%Y-%m-%d")
                },
                "æœ€è¿‘3å¤©": {
                    "start": (now - timedelta(days=3)).strftime("%Y-%m-%d"),
                    "end": now.strftime("%Y-%m-%d")
                },
                "æœ€è¿‘1å‘¨": {
                    "start": (now - timedelta(days=7)).strftime("%Y-%m-%d"),
                    "end": now.strftime("%Y-%m-%d")
                },
                "æœ€è¿‘1æœˆ": {
                    "start": (now - timedelta(days=30)).strftime("%Y-%m-%d"),
                    "end": now.strftime("%Y-%m-%d")
                },
                "æœ€è¿‘3æœˆ": {
                    "start": (now - timedelta(days=90)).strftime("%Y-%m-%d"),
                    "end": now.strftime("%Y-%m-%d")
                },
                "å¹´åˆè‡³ä»Š": {
                    "start": now.replace(month=1, day=1).strftime("%Y-%m-%d"),
                    "end": now.strftime("%Y-%m-%d")
                }
            },
            
            # LLMæç¤ºä¿¡æ¯
            "llm_context": {
                "description": "å½“å‰æœåŠ¡å™¨æ—¶é—´ä¿¡æ¯ï¼Œç”¨äºç†è§£ç”¨æˆ·çš„æ¨¡ç³Šæ—¶é—´è¡¨è¾¾",
                "usage_examples": [
                    "å½“ç”¨æˆ·è¯´'æœ€è¿‘'æ—¶ï¼Œé€šå¸¸æŒ‡æœ€è¿‘3å¤©",
                    "å½“ç”¨æˆ·è¯´'è¿‘æœŸ'æ—¶ï¼Œé€šå¸¸æŒ‡æœ€è¿‘1å‘¨", 
                    "å½“ç”¨æˆ·è¯´'è¿™å‡ å¤©'æ—¶ï¼Œé€šå¸¸æŒ‡æœ€è¿‘5å¤©",
                    "è‚¡ç¥¨æ•°æ®åˆ†æå»ºè®®ä½¿ç”¨äº¤æ˜“æ—¥æ—¶é—´èŒƒå›´"
                ]
            }
        }
    }


def _get_next_trading_day(current_time: datetime) -> datetime:
    """è®¡ç®—ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥"""
    next_day = current_time + timedelta(days=1)
    
    # è·³è¿‡å‘¨æœ«
    while next_day.weekday() >= 5:  # å‘¨å…­=5, å‘¨æ—¥=6
        next_day += timedelta(days=1)
    
    return next_day


async def _persist_dashboard_sessions():
    if not dashboard_session_store:
        return
    snapshot = {}
    async with _dashboard_lock:
        for session_id, info in dashboard_sessions.items():
            snapshot[session_id] = {
                "code": info.get("code"),
                "nickname": info.get("nickname"),
                "created_at": info.get("created_at")
            }
    await asyncio.to_thread(dashboard_session_store.save, snapshot)


def _remove_duplicate_sessions_locked() -> List[Tuple[str, str]]:
    """å¿…é¡»åœ¨æŒæœ‰_dashboard_lockæ—¶è°ƒç”¨ï¼ŒæŒ‰codeå»é‡å¹¶è¿”å›ç§»é™¤è®°å½•(session_id, code)"""
    seen_codes = set()
    duplicates: List[Tuple[str, str]] = []
    for session_id, info in list(dashboard_sessions.items()):
        code = info.get("code")
        if not code:
            continue
        if code in seen_codes:
            duplicates.append((session_id, code))
            dashboard_sessions.pop(session_id, None)
        else:
            seen_codes.add(code)
    return duplicates


def _get_session_window(code: str) -> Dict[str, Any]:
    market = "HK" if code.upper().startswith("HK.") else "US"
    tz_offset = 8 if market == "HK" else -5
    tz = timezone(timedelta(hours=tz_offset))
    today_local = datetime.now(tz).date()

    def local_iso(hour: int, minute: int) -> str:
        dt = datetime.combine(today_local, dtime(hour=hour, minute=minute, tzinfo=tz))
        return dt.astimezone(timezone.utc).isoformat()

    if market == "HK":
        window = {
            "market": "HK",
            "open_time": local_iso(9, 30),
            "break_start": local_iso(12, 0),
            "break_end": local_iso(13, 0),
            "close_time": local_iso(16, 0),
        }
    else:
        window = {
            "market": market,
            "open_time": local_iso(9, 30),
            "close_time": local_iso(16, 0),
        }
    return window


BULLISH_KEYWORDS = ["ä¸Šæ¶¨", "åˆ©å¥½", "å¢æŒ", "åˆ›æ–°é«˜", "è¶…é¢„æœŸ", "å¤§å¹…å¢é•¿", "è·æ‰¹", "å›è´­", "ä¸Šè°ƒ"]
BULLISH_KEYWORDS = [...]
BULLISH_KEYWORDS = ["ä¸Šæ¶¨", "åˆ©å¥½", "å¢æŒ", "åˆ›æ–°é«˜", "è¶…é¢„æœŸ", "å¤§å¹…å¢é•¿", "è·æ‰¹", "å›è´­", "ä¸Šè°ƒ"]
BEARISH_KEYWORDS = ["ä¸‹è·Œ", "åˆ©ç©º", "è­¦å‘Š", "äºæŸ", "è£å‘˜", "å¤§å¹…å‡å°‘", "ä¸‹è°ƒ", "å‡æŒ", "åœç‰Œ"]


def _normalize_datetime(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc)


def _parse_publish_time(value: Any) -> Optional[datetime]:
    if not value:
        return None
    if isinstance(value, datetime):
        return _normalize_datetime(value)
    if isinstance(value, (int, float)):
        try:
            ts = float(value)
            if ts > 1_000_000_000_000:
                ts = ts / 1000.0
            return datetime.fromtimestamp(ts, tz=timezone.utc)
        except Exception:
            return None
    if isinstance(value, str):
        raw = value.strip()
        if not raw:
            return None
        try:
            if raw.endswith("Z"):
                raw = raw[:-1] + "+00:00"
            return _normalize_datetime(datetime.fromisoformat(raw))
        except Exception:
            pass
        try:
            dt = parsedate_to_datetime(raw)
            return _normalize_datetime(dt)
        except Exception:
            pass
        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y-%m-%d"):
            try:
                dt = datetime.strptime(raw, fmt)
                return _normalize_datetime(dt)
            except Exception:
                continue
    return None


def _ensure_dashboard_session(session_id: str) -> Dict[str, Any]:
    session = dashboard_sessions.get(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="dashboard session not found")
    return session


async def _fetch_quote_snapshot(code: str) -> Optional[Dict[str, Any]]:
    if not futu_service:
        logger.warning(f"[quote.snapshot] futu_service_unavailable code={code}")
        return None
    request = StockQuoteRequest(code_list=[code])
    result = await futu_service.get_stock_quote(request)
    if result.ret_code != 0 or not result.data:
        logger.warning(f"[quote.snapshot] ret={result.ret_code} empty={not bool(result.data)} code={code}")
        return None
    quotes = result.data.get("quotes") or []
    if not quotes:
        logger.warning(f"[quote.snapshot] no_quotes code={code}")
        return None
    raw = quotes[0]

    def _to_float(value: Any) -> Optional[float]:
        if value in ("", None):
            return None
        if isinstance(value, (int, float)):
            try:
                as_float = float(value)
                if as_float != as_float:  # NaN
                    return None
                return as_float
            except (TypeError, ValueError):
                return None
        if isinstance(value, str):
            cleaned = value.strip()
            if not cleaned or cleaned in {"--", "N/A", "null", "-"}:
                return None
            cleaned = cleaned.replace(",", "")
            try:
                if cleaned.endswith("%"):
                    cleaned = cleaned.rstrip("%")
                return float(cleaned)
            except ValueError:
                return None
        try:
            return float(value)
        except (TypeError, ValueError):
            return None

    try:
        last_price = _to_float(raw.get("last_price") or raw.get("price"))
        prev_close = _to_float(raw.get("prev_close_price") or raw.get("prev_close"))
        change_rate = _to_float(raw.get("change_rate"))
        open_price = _to_float(raw.get("open_price") or raw.get("open"))
        high_price = _to_float(raw.get("high_price") or raw.get("high"))
        low_price = _to_float(raw.get("low_price") or raw.get("low"))
        volume = _to_float(raw.get("volume"))
        turnover = _to_float(raw.get("turnover"))
        change_value = None
        if last_price is not None and prev_close is not None:
            change_value = last_price - prev_close
            if (change_rate is None or change_rate == 0) and prev_close:
                try:
                    change_rate = (change_value / prev_close) * 100
                except ZeroDivisionError:
                    change_rate = None
        return {
            "code": raw.get("code"),
            "name": raw.get("stock_name") or raw.get("name"),
            "price": last_price,
            "last_price": last_price,
            "change_rate": change_rate,
            "change_value": change_value,
            "open": open_price,
            "high": high_price,
            "low": low_price,
            "prev_close": prev_close,
            "volume": volume,
            "turnover": turnover,
            "update_time": raw.get("update_time")
        }
    except Exception:
        return raw


async def _fetch_recommendations_snapshot(code: str) -> List[Dict[str, Any]]:
    if not recommendation_storage:
        return []
    req = RecommendationQueryRequest(code=code, limit=5)
    return await asyncio.to_thread(recommendation_storage.get_recommendations, req.dict())


async def _fetch_holding_snapshot(code: str) -> Optional[Dict[str, Any]]:
    if not futu_service or not futu_service.has_trade_connection():
        return None
    try:
        request = PositionListRequest(code=code, trd_env=TrdEnv.REAL)
        result = await futu_service.get_position_list(request)
        if result.ret_code != 0 or not result.data:
            return None
        positions = result.data.get("position_list") or []
        if not positions:
            return None
        pos = positions[0]
        qty = float(pos.get("qty") or 0)
        cost_price = float(pos.get("cost_price") or 0)
        last_price = pos.get("price") or pos.get("last_price")
        if last_price in (None, "", 0) and futu_service:
            quote = await _fetch_quote_snapshot(code)
            if quote and quote.get("price") is not None:
                last_price = quote["price"]
        last_price = float(last_price or 0)
        pl_val = float(pos.get("pl_val") or 0)
        pl_ratio = float(pos.get("pl_ratio") or 0)  # already percent
        return {
            "æŒä»“": qty,
            "æˆæœ¬": cost_price,
            "ç°ä»·": last_price,
            "ç›ˆäº": pl_val,
            "ç›ˆäºæ¯”ä¾‹": pl_ratio,
        }
    except Exception as exc:
        logger.debug(f"è·å–æŒä»“å¤±è´¥: {exc}")
        return None


async def _fetch_news_signals(code: str, size: int = 12, refresh: bool = True, days: int = 3) -> Dict[str, List[Dict[str, Any]]]:
    signals = {"bullish": [], "bearish": [], "neutral": [], "sector_news": [], "macro_news": []}
    run_remote = refresh and fundamental_service.is_configured()
    days = max(1, min(int(days or 3), 30))
    now_utc = datetime.now(timezone.utc)
    cutoff_dt = now_utc - timedelta(days=days)
    max_dt = now_utc + timedelta(days=1)  # é˜²æ­¢æœªæ¥æ—¥æœŸæ±¡æŸ“

    def _normalize_publish_time(value: Optional[Any]) -> Optional[str]:
        if value is None:
            return None
        if isinstance(value, (int, float)):
            ts = float(value)
            if ts > 1e12:
                ts = ts / 1000.0
            try:
                return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()
            except Exception:
                return None
        text = str(value).strip()
        if not text:
            return None
        lower = text.lower()
        # ç›¸å¯¹æ—¶é—´å¤„ç†
        if lower in {"åˆšåˆš", "åˆšæ‰", "just now", "now"}:
            return now_utc.isoformat()
        rel_match = re.match(r"(\d+)\s*(åˆ†é’Ÿ|åˆ†|å°æ—¶|å¤©|æ—¥)\s*å‰?$", text)
        if rel_match:
            amount = int(rel_match.group(1))
            unit = rel_match.group(2)
            delta_kwargs = {}
            if unit in {"åˆ†é’Ÿ", "åˆ†"}:
                delta_kwargs["minutes"] = amount
            elif unit in {"å°æ—¶"}:
                delta_kwargs["hours"] = amount
            elif unit in {"å¤©", "æ—¥"}:
                delta_kwargs["days"] = amount
            if delta_kwargs:
                return (now_utc - timedelta(**delta_kwargs)).isoformat()
        if lower in {"æ˜¨å¤©", "yesterday"}:
            return (now_utc - timedelta(days=1)).isoformat()
        # ç›´æ¥è§£ææ—¶é—´æˆ³/æ—¥æœŸ
        try:
            parsed = parsedate_to_datetime(text)
            if parsed.tzinfo is None:
                parsed = parsed.replace(tzinfo=timezone.utc)
            return parsed.astimezone(timezone.utc).isoformat()
        except Exception:
            pass
        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S", "%Y-%m-%d", "%Y/%m/%d", "%m-%d", "%m/%d"):
            try:
                dt = datetime.strptime(text, fmt)
                if fmt in ("%m-%d", "%m/%d"):
                    dt = dt.replace(year=now_utc.year)
                return dt.replace(tzinfo=timezone.utc).isoformat()
            except Exception:
                continue
        # æ‰‹å·¥æå–æ•°å­—æ—¥æœŸ
        digits = re.findall(r"\d+", text)
        if len(digits) >= 3:
            year, month, day = digits[0], digits[1], digits[2]
            hour = digits[3] if len(digits) > 3 else "0"
            minute = digits[4] if len(digits) > 4 else "0"
            second = digits[5] if len(digits) > 5 else "0"
            try:
                dt = datetime(
                    int(year),
                    int(month),
                    int(day),
                    int(hour),
                    int(minute),
                    int(second),
                    tzinfo=timezone.utc
                )
                return dt.isoformat()
            except Exception:
                pass
        return None


    async def _build_search_query() -> str:
        def _ensure_recent(q: str) -> str:
            text = q.lower()
            recency_token = f"æœ€è¿‘{days}å¤©"
            recent_tokens = [
                "æœ€è¿‘", "æœ¬å‘¨", "è¿‡å»", "è¿‘ä¸€å‘¨", "last 7 days", "past week", "today", "yesterday", "last week",
                recency_token.lower(), f"è¿‘{days}å¤©", f"{days}å¤©"
            ]
            if any(tok.lower() in text for tok in recent_tokens):
                return q
            return f"{q} {recency_token}"

        default_query = _ensure_recent(f"{code} æœ€æ–° è‚¡ç¥¨æ–°é—» åŸºæœ¬é¢")
        if not run_remote or not deepseek_service or not deepseek_service.is_configured():
            return default_query
        company_name = code
        try:
            quote_info = await _fetch_quote_snapshot(code)
            if quote_info and quote_info.get("name"):
                company_name = str(quote_info["name"])
        except Exception:
            company_name = code
        prompt = (
            "ä½ æ˜¯é‡‘èæƒ…æŠ¥æ£€ç´¢ä¸“å®¶ã€‚è¯·åŸºäºè‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°ï¼Œæä¾›ä¸€ä¸ªç”¨äºæ–°é—»æœç´¢çš„ç®€çŸ­ä¸­æ–‡å…³é”®è¯ï¼Œ"
            "èšç„¦åŸºæœ¬é¢ã€è´¢æŠ¥ã€æ”¿ç­–æˆ–ç«äº‰æƒ…æŠ¥ï¼Œä¸è¶…è¿‡18ä¸ªæ±‰å­—ã€‚"
            'è¾“å‡º JSONï¼Œå¦‚ {"query": "ç¾å›¢ åŸºæœ¬é¢ è´¢æŠ¥"}ã€‚'
            f"\nè‚¡ç¥¨ä»£ç : {code}\nå…¬å¸åç§°: {company_name}"
        )
        try:
            raw = await deepseek_service.chat("ä½ æ˜¯èµ„è®¯æ£€ç´¢ä¸“å®¶ã€‚", prompt, temperature=0.2)
            if raw:
                text = raw.strip().strip('`')
                if text.lower().startswith("json"):
                    text = text[4:].lstrip()
                data = json.loads(text)
                if isinstance(data, dict):
                    query = data.get("query")
                    if isinstance(query, str) and query.strip():
                        return _ensure_recent(query.strip())
        except Exception as exc:
            logger.debug(f"ç”Ÿæˆæ£€ç´¢å…³é”®è¯å¤±è´¥: {exc}")
        return default_query

    def _build_code_tokens(symbol: str) -> List[str]:
        base = symbol.upper()
        tokens = {base}
        if "." in base:
            market, num = base.split(".", 1)
            tokens.update({num, num.lstrip("0") or num, f"{market}{num}", f"{market}{num.lstrip('0') or num}", f"{num}.{market}", f"{num.lstrip('0') or num}.{market}"})
        tokens.update({base.replace(".", ""), base.replace(".", " ")})
        return [token for token in tokens if token]

    code_tokens = _build_code_tokens(code)
    search_query = await _build_search_query()

    def _has_code_token(record: Dict[str, Any]) -> bool:
        title = (record.get("title") or "").upper()
        snippet = (record.get("snippet") or "").upper()
        merged = f"{title} {snippet}"
        return any(token and token in merged for token in code_tokens)

    def _classify_scope(record: Dict[str, Any], analysis: Optional[Dict[str, Any]]) -> str:
        # ä¼˜å…ˆï¼šå¦‚æœå½“å‰è¯·æ±‚çš„ code ä¸è®°å½•çš„ code ä¸€è‡´ï¼Œç›´æ¥è§†ä¸º directï¼Œé˜²æ­¢è¢«è¯¯åˆ¤ä¸º sector/macro
        if record.get("code") and record.get("code") == code:
            return "direct"
        if analysis:
            scope = analysis.get("related_scope")
            if isinstance(scope, str):
                scope_lower = scope.lower()
                if scope_lower in {"direct", "sector", "macro"}:
                    if scope_lower == "direct" and not _has_code_token(record):
                        return "sector"
                    return scope_lower
            related_val = analysis.get("related")
            if isinstance(related_val, bool):
                if related_val and _has_code_token(record):
                    return "direct"
                if related_val:
                    return "sector"
                return "macro"
        if _has_code_token(record):
            return "direct"
        return "macro"

    def _fallback_analysis(text: str) -> Dict[str, Any]:
        lowered = text.lower()
        sentiment = "neutral"
        if any(key.lower() in lowered for key in BEARISH_KEYWORDS):
            sentiment = "bearish"
        elif any(key.lower() in lowered for key in BULLISH_KEYWORDS):
            sentiment = "bullish"
        scope = "direct" if any(token.lower() in lowered for token in code_tokens) else "macro"
        return {
            "sentiment": sentiment,
            "confidence": 0.4,
            "impact_horizon": "çŸ­æœŸ",
            "volatility_bias": "ä¸­æ€§",
            "themes": [],
            "risk_factors": [],
            "opportunity_factors": [],
            "summary": text[:100],
            "action_hint": "",
            "related": scope == "direct",
            "related_scope": scope,
            "analysis_provider": "fallback"
        }

    async def _ensure_analysis(item: Dict[str, Any]) -> Dict[str, Any]:
        payload = {
            "code": code,
            "title": item.get("title"),
            "snippet": item.get("snippet"),
            "source": item.get("source"),
            "publish_time": item.get("publish_time"),
            "url": item.get("url")
        }
        text = f"{payload['title'] or ''} {payload['snippet'] or ''}"
        analysis: Optional[Dict[str, Any]] = None
        if deepseek_service and deepseek_service.is_configured():
            try:
                analysis = await deepseek_service.analyze_fundamental_news(payload)
                if analysis is not None:
                    analysis["analysis_provider"] = "deepseek"
            except Exception as exc:
                logger.warning(
                    f"[fundamental.analysis] DeepSeek è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å…œåº•åˆ†æ code={code} title={payload['title']} err={exc}"
                )
        if not analysis:
            detail = None
            if deepseek_service:
                detail = getattr(deepseek_service, "last_error_message", None)
            logger.warning(
                "[fundamental.analysis] ä½¿ç”¨å…³é”®è¯å…œåº•æƒ…ç»ªï¼ŒDeepSeek æ— å“åº”æˆ–è§£æå¤±è´¥ code=%s title=%s detail=%s",
                code,
                payload["title"],
                detail or "æ— ",
            )
            analysis = _fallback_analysis(text)
            analysis["analysis_provider"] = "fallback"
        return analysis

    async def _repair_existing_records(quota: int = 5):
        if not fundamental_storage or not deepseek_service or not deepseek_service.is_configured():
            return
        pending_items = fundamental_storage.get_reanalysis_queue(code, limit=quota)
        if not pending_items:
            return
        logger.info(f"[fundamental.repair] code={code} pending={len(pending_items)} è§¦å‘é‡ç®—")
        for pending in pending_items:
            try:
                analysis = await _ensure_analysis(pending)
                pending["analysis"] = analysis
                fundamental_storage.upsert(pending)
            except Exception as exc:
                logger.warning(f"[fundamental.repair] é‡ç®—å¤±è´¥ code={code} title={pending.get('title')} err={exc}")

    try:
        request = FundamentalSearchRequest(
            q=search_query,
            scope="news",
            includeSummary=True,
            size=min(max(size, 10), 100),
            includeRawContent=False,
            conciseSnippet=True
        )
        response = await fundamental_service.search_fundamental_info(request)
        results = response.results or []
        fetched_count = len(results)
        kept_count = 0
        new_count = 0
        updated_count = 0
        skipped_no_time = 0
        skipped_cutoff = 0
        max_items = min(len(results), size)
        for item in results[:max_items]:
            raw_publish_time = item.publish_time
            normalized_publish_time = _normalize_publish_time(raw_publish_time)
            if cutoff_dt:
                if not normalized_publish_time:
                    # ç¼ºå¤±æˆ–æ— æ³•è§£æå‘å¸ƒæ—¶é—´ï¼Œè·³è¿‡ï¼Œé¿å…æ—¶é—´è½´è¢«æ—§æ•°æ®æ±¡æŸ“
                    skipped_no_time += 1
                    continue
                try:
                    dt = datetime.fromisoformat(normalized_publish_time.replace("Z", "+00:00"))
                    if dt < cutoff_dt or dt > max_dt:
                        skipped_cutoff += 1
                        continue
                except Exception:
                    skipped_no_time += 1
                    continue
            record = {
                "code": code,
                "title": item.title,
                "url": item.url,
                "source": item.source,
                "snippet": item.snippet,
                "raw_publish_time": raw_publish_time,
                "publish_time": normalized_publish_time or raw_publish_time or now_utc.isoformat(),
            }
            unique_key = None
            stored = None
            if fundamental_storage:
                unique_key = fundamental_storage.make_unique_key(
                    code,
                    item.title or "",
                    item.url or "",
                    normalized_publish_time or raw_publish_time
                )
                stored = fundamental_storage.get_by_unique_key(unique_key)
            analysis = stored.get("analysis") if stored and stored.get("analysis") else None
            if not analysis:
                analysis = await _ensure_analysis(record)
            scope = _classify_scope(record, analysis)
            if scope != "direct":
                logger.info(
                    f"[fundamental.filter] è¯†åˆ«ä¸º{scope}çº§èµ„è®¯ code={code} title={record.get('title')}"
                )
            record["analysis"] = analysis
            if fundamental_storage:
                record["unique_key"] = unique_key
                existed = bool(stored)
                fundamental_storage.upsert(record)
                if existed:
                    updated_count += 1
                else:
                    new_count += 1
            kept_count += 1
        logger.info(
            f"[fundamental.refresh] code={code} fetched={fetched_count} kept={kept_count} "
            f"new={new_count} updated={updated_count} "
            f"skip_no_time={skipped_no_time} skip_cutoff={skipped_cutoff} days={days}"
        )
    except Exception as exc:
        logger.debug(f"åŸºæœ¬é¢å¢é‡æœç´¢å¤±è´¥: {exc}")

    if run_remote:
        refresh_quota = max(3, min(10, max(size // 2, 1)))
        await _repair_existing_records(quota=refresh_quota)

    def _build_cached_news_signals() -> Dict[str, List[Dict[str, Any]]]:
        local_signals = {key: [] for key in signals.keys()}
        stored_records: List[Dict[str, Any]] = []
        sector_records: List[Dict[str, Any]] = []
        macro_records: List[Dict[str, Any]] = []
        preview_times: List[str] = []

        def _sort_key_by_time(rec: Dict[str, Any]):
            pt = rec.get("publish_time") or rec.get("last_seen") or rec.get("first_seen") or ""
            norm = _normalize_publish_time(pt) if pt else None
            try:
                return datetime.fromisoformat((norm or "").replace("Z", "+00:00"))
            except Exception:
                return datetime.min

        if fundamental_storage:
            raw_records = fundamental_storage.get_recent_news(code, limit=40)
            # æŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åºï¼Œé¿å…å­—ç¬¦ä¸²æ’åºå¯¼è‡´æ–°æ•°æ®è¢«åŸ‹
            raw_records_sorted = sorted(raw_records, key=_sort_key_by_time, reverse=True)
            for rec in raw_records_sorted:
                if rec.get("code") and rec.get("code") != code:
                    continue
                scope = _classify_scope(rec, rec.get("analysis"))
                if scope == "direct":
                    stored_records.append(rec)
                elif scope == "sector":
                    sector_records.append(rec)
                else:
                    macro_records.append(rec)
        if not stored_records:
            logger.info(f"[fundamental.signals.cache] code={code} æœ¬åœ°æ— directèµ„è®¯ï¼Œsector={len(sector_records)} macro={len(macro_records)}")
            local_signals["daily_metrics"] = []
            return local_signals
        logger.info(
            f"[fundamental.signals.cache.raw] code={code} direct={len(stored_records)} sector={len(sector_records)} macro={len(macro_records)}"
        )
        daily_buckets: Dict[str, Dict[str, Dict[str, float]]] = defaultdict(
            lambda: {
                "counts": {"bullish": 0.0, "bearish": 0.0, "neutral": 0.0},
                "weights": {"bullish": 0.0, "bearish": 0.0, "neutral": 0.0}
            }
        )
        for rec in stored_records:
            analysis = rec.get("analysis") or {}
            raw_sentiment = analysis.get("sentiment") or "neutral"
            sentiment = "neutral"
            if raw_sentiment in ("bullish", "åˆ©å¥½", "çœ‹æ¶¨"):
                sentiment = "bullish"
            elif raw_sentiment in ("bearish", "åˆ©ç©º", "çœ‹è·Œ"):
                sentiment = "bearish"
            publish_time_val = rec.get("publish_time") or rec.get("last_seen") or rec.get("first_seen")
            normalized_publish_time = _normalize_publish_time(publish_time_val) if publish_time_val else None
            if normalized_publish_time and len(preview_times) < 3:
                preview_times.append(normalized_publish_time)
            entry = {
                "title": rec.get("title"),
                "snippet": rec.get("snippet"),
                "source": rec.get("source"),
                "url": rec.get("url"),
                "publish_time": normalized_publish_time or publish_time_val,
                "published_at": normalized_publish_time or publish_time_val,
                "analysis": analysis
            }
            local_signals.setdefault(sentiment, []).append(entry)
            publish_date = (normalized_publish_time or publish_time_val or "")[:10]
            if publish_date:
                bucket = daily_buckets[publish_date]
                bucket["counts"][sentiment] += 1
                impact_score = float(analysis.get("impact_score") or 50)
                novelty_score = float(analysis.get("novelty_score") or analysis.get("magnitude_score") or 50)
                weight = max(0.1, min((impact_score * 0.7 + novelty_score * 0.3) / 100, 2.0))
                if analysis.get("effectiveness") == "stale":
                    weight *= 0.3
                elif analysis.get("effectiveness") == "diminished":
                    weight *= 0.6
                bucket["weights"][sentiment] += weight
        daily_metrics = []
        for date, bucket in sorted(daily_buckets.items(), key=lambda x: x[0], reverse=True):
            counts = bucket["counts"]
            weights = bucket["weights"]
            total = counts["bullish"] + counts["bearish"] + counts["neutral"]
            total_weight = weights["bullish"] + weights["bearish"] + weights["neutral"]
            if total == 0:
                continue
            weight_score = (weights["bullish"] - weights["bearish"]) / total_weight if total_weight else 0.0
            score = (counts["bullish"] - counts["bearish"]) / total
            daily_metrics.append({
                "date": date,
                "bullish": counts["bullish"],
                "bearish": counts["bearish"],
                "neutral": counts["neutral"],
                "score": round(score, 2),
                "weighted_score": round(weight_score, 2)
            })
        local_signals["daily_metrics"] = daily_metrics
        local_signals["sector_news"] = [
            {
                "title": rec.get("title"),
                "snippet": rec.get("snippet"),
                "source": rec.get("source"),
                "url": rec.get("url"),
                "publish_time": _normalize_publish_time(rec.get("publish_time") or rec.get("last_seen") or rec.get("first_seen")) or rec.get("publish_time"),
                "analysis": rec.get("analysis"),
            }
            for rec in sector_records
        ]
        local_signals["macro_news"] = [
            {
                "title": rec.get("title"),
                "snippet": rec.get("snippet"),
                "source": rec.get("source"),
                "url": rec.get("url"),
                "publish_time": _normalize_publish_time(rec.get("publish_time") or rec.get("last_seen") or rec.get("first_seen")) or rec.get("publish_time"),
                "analysis": rec.get("analysis"),
            }
            for rec in macro_records
        ]
        logger.info(
            f"[fundamental.signals.cache] code={code} bullish={len(local_signals.get('bullish', []))} "
            f"bearish={len(local_signals.get('bearish', []))} total_direct={len(stored_records)} "
            f"sector={len(sector_records)} macro={len(macro_records)} "
            f"top_publish_time={preview_times}"
        )
        return local_signals

    return _build_cached_news_signals()

def _normalize_kline_cache_scope(request: HistoryKLineRequest) -> Tuple[str, str, str]:
    """ç”Ÿæˆç”¨äºç¼“å­˜çš„Kçº¿èŒƒå›´æ ‡è¯†ï¼Œé¿å…ä¸åŒè¯·æ±‚äº’ç›¸æ±¡æŸ“"""
    ktype_value = request.ktype.value if hasattr(request.ktype, "value") else str(request.ktype)
    autype_value = request.autype.value if hasattr(request.autype, "value") else str(request.autype)
    start_token = request.start or f"recent:{request.max_count}"
    end_token = request.end or "latest"
    return f"{ktype_value}:{autype_value}", start_token, end_token


# ==================== åŸæœ‰è¡Œæƒ…æ¥å£ï¼ˆå¢å¼ºç‰ˆï¼‰ ====================
@app.post("/api/quote/history_kline",
          operation_id="get_history_kline_enhanced",
          summary="è·å–å†å²Kçº¿ï¼ˆç¼“å­˜å¢å¼ºï¼‰",
          description="æ™ºèƒ½ç¼“å­˜çš„å†å²Kçº¿è·å–ï¼Œè‡ªåŠ¨ä»ç¼“å­˜ä¼˜åŒ–æ•°æ®è·å–é€Ÿåº¦")
async def get_history_kline_enhanced(request: HistoryKLineRequest) -> APIResponse:
    """è·å–å†å²Kçº¿æ•°æ®ï¼ˆç¼“å­˜å¢å¼ºç‰ˆï¼‰"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    start_time = time.time()
    cache_hit = False
    ktype_token, cache_start, cache_end = _normalize_kline_cache_scope(request)
    
    try:
        # 1. å°è¯•ä»ç¼“å­˜è·å–æ•°æ®
        if cache_manager:
            cached_data = await cache_manager.get_kline_data(
                request.code, ktype_token, cache_start, cache_end
            )
            if cached_data:
                cache_hit = True
                logger.info(f"ç¼“å­˜å‘½ä¸­: {request.code} {request.ktype.value}")
                
                execution_time = time.time() - start_time
                return APIResponse(
                    ret_code=0,
                    ret_msg=f"è·å–å†å²Kçº¿æˆåŠŸï¼ˆç¼“å­˜ï¼‰- æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s",
                    data={
                        "kline_data": cached_data,
                        "data_count": len(cached_data),
                        "cache_hit": True,
                        "execution_time": execution_time
                    }
                )
        
        # 2. ä»APIè·å–æ•°æ®
        result = await futu_service.get_history_kline(request)
        
        # 3. å­˜å‚¨åˆ°ç¼“å­˜
        if result.ret_code == 0 and cache_manager and result.data.get("kline_data"):
            await cache_manager.store_kline_data(
                request.code, ktype_token,
                cache_start, cache_end,
                result.data["kline_data"]
            )
        
        execution_time = time.time() - start_time
        
        # å¢å¼ºè¿”å›æ•°æ®
        if result.ret_code == 0 and result.data:
            result.data.update({
                "cache_hit": cache_hit,
                "execution_time": execution_time,
                "data_source": "futu_api"
            })
            result.ret_msg += f" - æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s"
        
        return result
        
    except Exception as e:
        logger.exception(f"è·å–å†å²Kçº¿å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–å†å²Kçº¿å¼‚å¸¸: {e}", data=None)


@app.post("/api/quote/stock_quote",
          operation_id="get_stock_quote_enhanced", 
          summary="è·å–è‚¡ç¥¨æŠ¥ä»·ï¼ˆç¼“å­˜å¢å¼ºï¼‰")
async def get_stock_quote_enhanced(request: StockQuoteRequest) -> APIResponse:
    """è·å–è‚¡ç¥¨æŠ¥ä»·ï¼ˆç¼“å­˜å¢å¼ºç‰ˆï¼‰"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    start_time = time.time()
    cache_hit = False
    
    try:
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        if cache_manager:
            cached_data = await cache_manager.get_quote_data(request.code_list)
            if cached_data:
                cache_hit = True
                execution_time = time.time() - start_time
                return APIResponse(
                    ret_code=0,
                    ret_msg=f"è·å–è‚¡ç¥¨æŠ¥ä»·æˆåŠŸï¼ˆç¼“å­˜ï¼‰- æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s",
                    data={
                        "quotes": cached_data,
                        "data_count": len(cached_data),
                        "cache_hit": True,
                        "execution_time": execution_time
                    }
                )
        
        # 2. ä»APIè·å–
        result = await futu_service.get_stock_quote(request)
        
        # 3. å­˜å‚¨åˆ°ç¼“å­˜
        if result.ret_code == 0 and cache_manager and result.data.get("quotes"):
            await cache_manager.store_quote_data(request.code_list, result.data["quotes"])
        
        execution_time = time.time() - start_time
        if result.ret_code == 0 and result.data:
            result.data.update({
                "cache_hit": cache_hit,
                "execution_time": execution_time
            })
        
        return result
        
    except Exception as e:
        logger.exception(f"è·å–è‚¡ç¥¨æŠ¥ä»·å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–è‚¡ç¥¨æŠ¥ä»·å¼‚å¸¸: {e}", data=None)


# ==================== æŠ€æœ¯åˆ†ææ¥å£ ====================
@app.post("/api/analysis/technical_indicators",
          operation_id="get_technical_indicators",
          summary="è·å–æŠ€æœ¯åˆ†ææŒ‡æ ‡",
          description="è®¡ç®—MACDã€RSIã€å¸ƒæ—å¸¦ç­‰æŠ€æœ¯æŒ‡æ ‡ï¼Œæ”¯æŒç¼“å­˜ä¼˜åŒ–")
async def get_technical_indicators(request: TechnicalAnalysisRequest) -> Dict[str, Any]:
    """è·å–æŠ€æœ¯åˆ†ææŒ‡æ ‡"""
    start_time = time.time()
    cache_hit = False
    
    logger.info(f"ğŸ” å¼€å§‹å¤„ç†æŠ€æœ¯åˆ†æè¯·æ±‚: {request.code}, æŒ‡æ ‡: {request.indicators}")
    
    try:
        # 1. æ£€æŸ¥æŒ‡æ ‡ç¼“å­˜
        if cache_manager:
            cached_indicators = await cache_manager.get_indicator_data(
                "comprehensive", request.code, request.dict()
            )
            if cached_indicators:
                cache_hit = True
                execution_time = time.time() - start_time
                
                return {
                    "ret_code": 0,
                    "ret_msg": "æŠ€æœ¯åˆ†æè·å–æˆåŠŸï¼ˆç¼“å­˜ï¼‰",
                    "data": cached_indicators,
                    "execution_time": execution_time,
                    "cache_hit": True,
                    "data_source": "cache",
                    "timestamp": datetime.now().isoformat()
                }
        
        # 2. è·å–Kçº¿æ•°æ®ï¼ˆä¼˜å…ˆä»ç¼“å­˜ï¼‰
        end_date = datetime.now().strftime('%Y-%m-%d')
        
        # æ ¹æ®Kçº¿ç±»å‹æ™ºèƒ½è®¡ç®—éœ€è¦çš„æ•°æ®é‡å’Œæ—¶é—´èŒƒå›´
        ktype_multipliers = {
            "K_1M": 1440,     # 1åˆ†é’Ÿ: 1å¤©=1440æ ¹Kçº¿
            "K_5M": 288,      # 5åˆ†é’Ÿ: 1å¤©=288æ ¹Kçº¿  
            "K_15M": 96,      # 15åˆ†é’Ÿ: 1å¤©=96æ ¹Kçº¿
            "K_30M": 48,      # 30åˆ†é’Ÿ: 1å¤©=48æ ¹Kçº¿
            "K_60M": 24,      # 60åˆ†é’Ÿ: 1å¤©=24æ ¹Kçº¿
            "K_DAY": 1,       # æ—¥çº¿: 1å¤©=1æ ¹Kçº¿
            "K_WEEK": 0.2,    # å‘¨çº¿: 1å¤©=0.2æ ¹Kçº¿
            "K_MON": 0.05     # æœˆçº¿: 1å¤©=0.05æ ¹Kçº¿
        }
        
        ktype_str = request.ktype.value if hasattr(request.ktype, 'value') else str(request.ktype)
        multiplier = ktype_multipliers.get(ktype_str, 1)
        
        # è®¡ç®—éœ€è¦çš„æœ€å°æ•°æ®é‡ï¼ˆè€ƒè™‘æŠ€æœ¯æŒ‡æ ‡è®¡ç®—éœ€æ±‚ï¼‰
        min_required_points = max(
            request.macd_slow + 10,  # MACDéœ€è¦æ…¢çº¿å‘¨æœŸ+é¢å¤–ç¼“å†²
            request.rsi_period + 10,  # RSIéœ€è¦å‘¨æœŸ+ç¼“å†²
            request.bollinger_period + 10,  # å¸ƒæ—å¸¦éœ€è¦å‘¨æœŸ+ç¼“å†²
            50  # æœ€å°‘50ä¸ªæ•°æ®ç‚¹
        )
        
        # æ ¹æ®Kçº¿é¢‘ç‡è®¡ç®—éœ€è¦çš„å¤©æ•°
        if multiplier > 1:  # åˆ†é’Ÿçº¿
            days_needed = max(
                int(min_required_points / multiplier) + 15,  # åŸºäºæ•°æ®ç‚¹è®¡ç®—+æ›´å¤§ç¼“å†²
                30  # è‡³å°‘30å¤©ï¼Œå¯¹äº30åˆ†é’Ÿçº¿ç¡®ä¿è¶³å¤Ÿæ•°æ®
            )
        else:  # æ—¥çº¿ã€å‘¨çº¿ã€æœˆçº¿
            days_needed = max(
                int(min_required_points / multiplier) + 20,
                90  # è‡³å°‘90å¤©
            )
        
        start_date = (datetime.now() - timedelta(days=days_needed)).strftime('%Y-%m-%d')
        max_count = min(1000, int(min_required_points * 2))  # å¢åŠ åˆ°2å€ç¼“å†²å¹¶æé«˜ä¸Šé™
        
        logger.info(f"ğŸ“Š å‡†å¤‡è·å–Kçº¿æ•°æ®: {request.code}, {start_date} ~ {end_date}")
        logger.info(f"ğŸ“Š Kçº¿ç±»å‹: {ktype_str}, é¢„è®¡éœ€è¦: {min_required_points}ä¸ªæ•°æ®ç‚¹, æŸ¥è¯¢å¤©æ•°: {days_needed}, max_count: {max_count}")
        
        kline_request = HistoryKLineRequest(
            code=request.code,
            start=start_date,
            end=end_date,
            ktype=request.ktype,
            max_count=max_count,
            optimization=request.optimization
        )
        
        logger.info(f"ğŸ“ è°ƒç”¨Kçº¿API...")
        kline_result = await get_history_kline_enhanced(kline_request)
        logger.info(f"ğŸ“ˆ Kçº¿APIè¿”å›: {kline_result.ret_code}, {kline_result.ret_msg}")
        
        if kline_result.ret_code != 0:
            return {
                "ret_code": kline_result.ret_code,
                "ret_msg": f"è·å–Kçº¿æ•°æ®å¤±è´¥: {kline_result.ret_msg}",
                "data": None
            }
        
        kline_data = kline_result.data.get("kline_data", [])
        if not kline_data:
            return {
                "ret_code": -1,
                "ret_msg": "Kçº¿æ•°æ®ä¸ºç©º",
                "data": None
            }
        
        # 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        logger.info(f"âš™ï¸  å¼€å§‹è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ŒKçº¿æ•°æ®é‡: {len(kline_data)}")
        
        config = IndicatorConfig(
            macd_fast=request.macd_fast,
            macd_slow=request.macd_slow,
            macd_signal=request.macd_signal,
            rsi_period=request.rsi_period,
            rsi_overbought=request.rsi_overbought,
            rsi_oversold=request.rsi_oversold,
            bollinger_period=request.bollinger_period,
            bollinger_std=request.bollinger_std,
            ma_periods=request.ma_periods
        )
        
        logger.info("ğŸ§® åˆ›å»ºæŠ€æœ¯åˆ†æå¯¹è±¡...")
        technical_data = TechnicalIndicators.from_kline_data(kline_data, config)
        logger.info("ğŸ“ˆ è®¡ç®—æ‰€æœ‰æŒ‡æ ‡...")
        indicators = technical_data.calculate_all_indicators()
        logger.info(f"âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆï¼ŒåŒ…å«: {list(indicators.keys())}")
        
        # 4. æ„å»ºç®€åŒ–ä¸”LLMå‹å¥½çš„å“åº”æ•°æ®
        logger.info("ğŸ“‹ æ„å»ºç®€åŒ–å“åº”æ•°æ®...")
        
        # æå–å…³é”®æŒ‡æ ‡çš„å½“å‰å€¼å’Œä¿¡å·ï¼Œé¿å…è¿‡é•¿çš„å†å²æ•°æ®
        simplified_data = _create_simplified_response(indicators, request.code, len(kline_data))
        
        logger.info(f"ğŸ” ç®€åŒ–åæ•°æ®ç»“æ„: {list(simplified_data.keys()) if isinstance(simplified_data, dict) else type(simplified_data)}")
        
        response_data = {
            "code": request.code,
            "period": request.period,
            "data_points": len(kline_data),
            "indicators": simplified_data,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info("âœ… å“åº”æ•°æ®æ„å»ºå®Œæˆ")
        
        # 5. å­˜å‚¨æŒ‡æ ‡åˆ°ç¼“å­˜
        if cache_manager:
            await cache_manager.store_indicator_data(
                "comprehensive", request.code, request.model_dump(), response_data
            )
        
        execution_time = time.time() - start_time
        
        # ä½¿ç”¨é€šç”¨çš„å­—å…¸å“åº”æ ¼å¼ï¼Œé¿å…ä¸¥æ ¼çš„æ¨¡å‹éªŒè¯
        return {
            "ret_code": 0,
            "ret_msg": "æŠ€æœ¯åˆ†æè®¡ç®—å®Œæˆ",
            "data": response_data,
            "execution_time": execution_time,
            "cache_hit": cache_hit,
            "data_source": "calculated",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.exception(f"æŠ€æœ¯åˆ†æè®¡ç®—å¤±è´¥: {e}")
        return {
            "ret_code": -1,
            "ret_msg": f"æŠ€æœ¯åˆ†æå¼‚å¸¸: {e}",
            "data": None
        }


def _create_simplified_response(indicators: Dict[str, Any], code: str, data_points: int) -> Dict[str, Any]:
    """åˆ›å»ºç®€åŒ–çš„LLMå‹å¥½å“åº”æ ¼å¼
    
    åªä¿ç•™å…³é”®ä¿¡æ¯ï¼š
    - å½“å‰å€¼ (current)
    - ä¿¡å· (signal) 
    - ç®€è¦æè¿°
    é¿å…è¿‡é•¿çš„å†å²æ•°æ®åºåˆ—
    """
    
    def extract_current_value(indicator_data):
        """æå–æŒ‡æ ‡çš„å½“å‰å€¼"""
        if not isinstance(indicator_data, dict):
            # logger.debug(f"ğŸ” æŒ‡æ ‡æ•°æ®ä¸æ˜¯å­—å…¸: {type(indicator_data)}")
            return None
            
        current = indicator_data.get("current")
        # logger.debug(f"ğŸ” å½“å‰å€¼åŸå§‹æ•°æ®: {current} (ç±»å‹: {type(current)})")
        
        if isinstance(current, dict):
            # è¿‡æ»¤æœ‰æ•ˆçš„æ•°å€¼
            valid_current = {k: v for k, v in current.items() 
                           if v is not None and isinstance(v, (int, float)) and not (isinstance(v, float) and (v != v or abs(v) == float('inf')))}
            # logger.debug(f"ğŸ” è¿‡æ»¤åçš„å½“å‰å€¼: {valid_current}")
            return valid_current if valid_current else None
        elif isinstance(current, (int, float)) and not (isinstance(current, float) and (current != current or abs(current) == float('inf'))):
            return current
        return None
    
    def extract_signal(indicator_data):
        """æå–æŒ‡æ ‡ä¿¡å·"""
        if isinstance(indicator_data, dict):
            signal = indicator_data.get("signal")
            # logger.debug(f"ğŸ” ä¿¡å·æ•°æ®: {signal}")
            return signal
        return None
    
    simplified = {}
    
    # å¤„ç†è¶‹åŠ¿æŒ‡æ ‡ (MACD, MA, EMAç­‰)
    if "trend_indicators" in indicators and indicators["trend_indicators"]:
        # logger.info("ğŸ” å¤„ç†è¶‹åŠ¿æŒ‡æ ‡...")
        trend_data = {}
        trend_indicators = indicators["trend_indicators"]
        
        # MACD
        if "macd" in trend_indicators:
            # logger.info("ğŸ” å¤„ç†MACD...")
            macd = trend_indicators["macd"]
            # logger.debug(f"ğŸ” MACDåŸå§‹æ•°æ®: {macd}")
            current_macd = extract_current_value(macd)
            if current_macd:
                trend_data["macd"] = {
                    "current": current_macd,
                    "signal": extract_signal(macd) or "ä¸­æ€§",
                    "description": "MACDåŠ¨é‡æŒ‡æ ‡"
                }
                # logger.info(f"ğŸ” MACDå¤„ç†æˆåŠŸ: {trend_data['macd']}")
            # else:
                # logger.warning("ğŸ” MACDå½“å‰å€¼ä¸ºç©º")
        
        # ç§»åŠ¨å¹³å‡çº¿
        if "moving_averages" in trend_indicators:
            # logger.info("ğŸ” å¤„ç†ç§»åŠ¨å¹³å‡çº¿...")
            ma = trend_indicators["moving_averages"]
            # logger.debug(f"ğŸ” MAåŸå§‹æ•°æ®: {ma}")
            current_ma = extract_current_value(ma)
            if current_ma:
                trend_data["moving_averages"] = {
                    "current": current_ma,
                    "signal": extract_signal(ma) or "ä¸­æ€§",
                    "description": "ç§»åŠ¨å¹³å‡çº¿"
                }
                # logger.info(f"ğŸ” MAå¤„ç†æˆåŠŸ: {trend_data['moving_averages']}")
            # else:
                # logger.warning("ğŸ” MAå½“å‰å€¼ä¸ºç©º")
        
        if trend_data:
            simplified["trend_indicators"] = trend_data
            # logger.info(f"ğŸ” è¶‹åŠ¿æŒ‡æ ‡å¤„ç†å®Œæˆ: {list(trend_data.keys())}")
    
    # å¤„ç†åŠ¨é‡æŒ‡æ ‡ (RSI, KDJç­‰)
    if "momentum_indicators" in indicators and indicators["momentum_indicators"]:
        # logger.info("ğŸ” å¤„ç†åŠ¨é‡æŒ‡æ ‡...")
        momentum_data = {}
        momentum_indicators = indicators["momentum_indicators"]
        
        # RSI
        if "rsi" in momentum_indicators:
            # logger.info("ğŸ” å¤„ç†RSI...")
            rsi = momentum_indicators["rsi"]
            # logger.debug(f"ğŸ” RSIåŸå§‹æ•°æ®: {rsi}")
            current_rsi = extract_current_value(rsi)
            if current_rsi:
                momentum_data["rsi"] = {
                    "current": current_rsi,
                    "signal": extract_signal(rsi) or "ä¸­æ€§",
                    "description": "ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡"
                }
                # logger.info(f"ğŸ” RSIå¤„ç†æˆåŠŸ: {momentum_data['rsi']}")
            # else:
                # logger.warning("ğŸ” RSIå½“å‰å€¼ä¸ºç©º")
        
        if momentum_data:
            simplified["momentum_indicators"] = momentum_data
            # logger.info(f"ğŸ” åŠ¨é‡æŒ‡æ ‡å¤„ç†å®Œæˆ: {list(momentum_data.keys())}")
    
    # å¤„ç†æŠ€æœ¯åˆ†ææ€»ç»“
    if "summary" in indicators and indicators["summary"]:
        # logger.info("ğŸ” å¤„ç†æŠ€æœ¯åˆ†ææ€»ç»“...")
        summary = indicators["summary"]
        # logger.debug(f"ğŸ” SummaryåŸå§‹æ•°æ®: {summary}")
        if isinstance(summary, dict):
            clean_summary = {k: v for k, v in summary.items() if v is not None and v != ""}
            if clean_summary:
                simplified["summary"] = clean_summary
                # logger.info(f"ğŸ” Summaryå¤„ç†æˆåŠŸ: {clean_summary}")
    
    # å¦‚æœæ‰€æœ‰æŒ‡æ ‡éƒ½æ— æ•ˆï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
    if not simplified:
        logger.warning("ğŸ” æ‰€æœ‰æŒ‡æ ‡éƒ½æ— æ•ˆï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯")
        simplified = {
            "status": "æ•°æ®è®¡ç®—ä¸­ï¼Œéƒ¨åˆ†æŒ‡æ ‡å¯èƒ½éœ€è¦æ›´å¤šå†å²æ•°æ®",
            "data_points": data_points,
            "note": "æŠ€æœ¯æŒ‡æ ‡éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®æ‰èƒ½è®¡ç®—å‡†ç¡®"
        }
    
    logger.info(f"âœ… ç®€åŒ–æ•°æ®åŒ…å«: {list(simplified.keys())}")
    return simplified


def _clean_indicator_data(indicators: Dict[str, Any]) -> Dict[str, Any]:
    """æ¸…ç†æŒ‡æ ‡æ•°æ®ä¸­çš„Noneå€¼ï¼Œç¡®ä¿PydanticéªŒè¯é€šè¿‡"""
    
    def clean_dict_values(data: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’æ¸…ç†å­—å…¸ä¸­çš„Noneå€¼"""
        if not isinstance(data, dict):
            return data
            
        cleaned = {}
        for key, value in data.items():
            if isinstance(value, dict):
                # é€’å½’æ¸…ç†åµŒå¥—å­—å…¸
                cleaned_value = clean_dict_values(value)
                # åªä¿ç•™åŒ…å«æœ‰æ•ˆæ•°æ®çš„å­—å…¸
                if cleaned_value:
                    cleaned[key] = cleaned_value
            elif isinstance(value, list):
                # æ¸…ç†åˆ—è¡¨ï¼Œç§»é™¤Noneå€¼
                cleaned_list = [v for v in value if v is not None]
                if cleaned_list:
                    cleaned[key] = cleaned_list
            elif value is not None:
                # ä¿ç•™éNoneå€¼
                cleaned[key] = value
            # è·³è¿‡Noneå€¼
                
        return cleaned
    
    def clean_indicator_structure(category_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†æŒ‡æ ‡ç±»åˆ«æ•°æ®ç»“æ„"""
        if not isinstance(category_data, dict):
            return category_data
            
        cleaned_category = {}
        
        for indicator_name, indicator_data in category_data.items():
            if not isinstance(indicator_data, dict):
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œç›´æ¥ä¿ç•™ï¼ˆå¦‚signalå­—ç¬¦ä¸²ï¼‰
                cleaned_category[indicator_name] = indicator_data
                continue
                
            cleaned_indicator = {}
            
            # å¤„ç†valueså­—æ®µ
            if "values" in indicator_data:
                values = indicator_data["values"]
                if isinstance(values, dict):
                    cleaned_values = clean_dict_values(values)
                    if cleaned_values:
                        cleaned_indicator["values"] = cleaned_values
                elif isinstance(values, list):
                    cleaned_values = [v for v in values if v is not None]
                    if cleaned_values:
                        cleaned_indicator["values"] = cleaned_values
                elif values is not None:
                    cleaned_indicator["values"] = values
            
            # å¤„ç†currentå­—æ®µ - è¿™æ˜¯å…³é”®å­—æ®µ
            if "current" in indicator_data:
                current = indicator_data["current"]
                if isinstance(current, dict):
                    # è¿‡æ»¤æ‰Noneå€¼ï¼Œåªä¿ç•™æœ‰æ•ˆçš„floatå€¼
                    cleaned_current = {k: v for k, v in current.items() if v is not None and isinstance(v, (int, float))}
                    if cleaned_current:
                        cleaned_indicator["current"] = cleaned_current
                elif current is not None and isinstance(current, (int, float)):
                    cleaned_indicator["current"] = current
                # å¦‚æœcurrentå…¨æ˜¯Noneæˆ–æ— æ•ˆï¼Œåˆ™è®¾ä¸ºNoneï¼Œè¿™æ ·Pydanticä¼šæ¥å—Optionalç±»å‹
                if "current" not in cleaned_indicator:
                    cleaned_indicator["current"] = None
            
            # å¤„ç†å…¶ä»–å­—æ®µ
            for field, value in indicator_data.items():
                if field not in ["values", "current"]:
                    if value is not None:
                        cleaned_indicator[field] = value
            
            # åªæœ‰å½“æ¸…ç†åçš„æŒ‡æ ‡æ•°æ®éç©ºæ—¶æ‰ä¿ç•™
            if cleaned_indicator:
                cleaned_category[indicator_name] = cleaned_indicator
        
        return cleaned_category
    
    # æ¸…ç†æ‰€æœ‰ç±»åˆ«çš„æŒ‡æ ‡æ•°æ®
    cleaned_indicators = {}
    
    for category, category_data in indicators.items():
        if category == "summary":
            # summaryæ˜¯ç®€å•çš„å­—ç¬¦ä¸²å­—å…¸ï¼Œç›´æ¥ä¿ç•™
            if isinstance(category_data, dict):
                cleaned_summary = {k: v for k, v in category_data.items() if v is not None}
                if cleaned_summary:
                    cleaned_indicators[category] = cleaned_summary
        else:
            # æ¸…ç†æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            cleaned_category = clean_indicator_structure(category_data)
            if cleaned_category:
                cleaned_indicators[category] = cleaned_category
    
    return cleaned_indicators


@app.post("/api/analysis/macd",
          operation_id="get_macd_indicator",
          summary="è·å–MACDæŒ‡æ ‡")
async def get_macd_indicator(request: TechnicalAnalysisRequest) -> Dict[str, Any]:
    """å•ç‹¬è·å–MACDæŒ‡æ ‡"""
    # å¤ç”¨comprehensiveæ¥å£ä½†åªè¿”å›MACD
    request.indicators = [IndicatorType.MACD]
    return await get_technical_indicators(request)


@app.post("/api/analysis/rsi",
          operation_id="get_rsi_indicator", 
          summary="è·å–RSIæŒ‡æ ‡")
async def get_rsi_indicator(request: TechnicalAnalysisRequest) -> Dict[str, Any]:
    """å•ç‹¬è·å–RSIæŒ‡æ ‡"""
    request.indicators = [IndicatorType.RSI]
    return await get_technical_indicators(request)


# ==================== ç¼“å­˜ç®¡ç†æ¥å£ ====================
@app.post("/api/system/request_quote_rights",
          operation_id="request_quote_rights", 
          summary="ğŸ”§ è¯·æ±‚æœ€é«˜è¡Œæƒ…æƒé™",
          description="æ‰‹åŠ¨è¯·æ±‚æœ€é«˜è¡Œæƒ…æƒé™ï¼Œè§£å†³æƒé™è¢«æŠ¢å çš„é—®é¢˜")
async def request_quote_rights() -> Dict[str, Any]:
    """æ‰‹åŠ¨è¯·æ±‚æœ€é«˜è¡Œæƒ…æƒé™"""
    if not _server_ready or not futu_service:
        return {
            "success": False,
            "message": "æœåŠ¡æœªå°±ç»ª",
            "timestamp": datetime.now().isoformat()
        }
    
    try:
        # å¼ºåˆ¶æ£€æŸ¥å¹¶è¯·æ±‚æƒé™
        logger.info("ğŸ“ æ¥æ”¶åˆ°æ‰‹åŠ¨æƒé™è¯·æ±‚...")
        success = await futu_service._check_and_ensure_quote_rights(force_check=True)
        
        return {
            "success": success,
            "message": "æƒé™è¯·æ±‚æˆåŠŸ" if success else "æƒé™è¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥OpenDè¿æ¥å’ŒTelnetç«¯å£",
            "rights_checked": futu_service._quote_rights_checked,
            "last_check_time": futu_service._last_quote_rights_check,
            "auto_request_enabled": futu_service._quote_rights_auto_request,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æƒé™è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return {
            "success": False,
            "message": f"æƒé™è¯·æ±‚å¼‚å¸¸: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }


@app.get("/api/cache/status",
         operation_id="get_cache_status",
         summary="è·å–ç¼“å­˜çŠ¶æ€")
async def get_cache_status(detailed: bool = False) -> Dict[str, Any]:
    """è·å–ç¼“å­˜çŠ¶æ€"""
    try:
        if not cache_manager:
            return {
                "ret_code": -1,
                "ret_msg": "ç¼“å­˜ç®¡ç†å™¨æœªåˆå§‹åŒ–",
                "data": None
            }
        
        stats = await cache_manager.get_cache_stats()
        
        # è®¡ç®—ä½¿ç”¨ç‡
        memory_usage_ratio = stats.get("memory_cache_size", 0) / max(stats.get("memory_max_size", 1), 1)
        
        # å¥åº·çŠ¶æ€è¯„ä¼°
        health_status = "healthy"
        recommendations = []
        
        if memory_usage_ratio > 0.9:
            health_status = "warning"
            recommendations.append("å†…å­˜ç¼“å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ¸…ç†")
        
        if not stats.get("redis_available", False):
            health_status = "degraded"
            recommendations.append("Redisä¸å¯ç”¨ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥")
        
        cache_stats = CacheStats(
            memory_cache_size=stats.get("memory_cache_size", 0),
            memory_max_size=stats.get("memory_max_size", 0),
            memory_usage_ratio=memory_usage_ratio,
            redis_available=stats.get("redis_available", False),
            redis_connected=stats.get("redis_connected"),
            redis_memory_usage=stats.get("redis_memory_usage"),
            sqlite_available=stats.get("sqlite_available", False),
            sqlite_kline_count=stats.get("sqlite_kline_count"),
            sqlite_indicator_count=stats.get("sqlite_indicator_count")
        )
        
        response_data = CacheStatusResponse(
            stats=cache_stats,
            detailed_info=stats if detailed else None,
            health_status=health_status,
            recommendations=recommendations
        )
        
        return {
            "ret_code": 0,
            "ret_msg": "ç¼“å­˜çŠ¶æ€è·å–æˆåŠŸ",
            "data": response_data,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.exception(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        return {
            "ret_code": -1,
            "ret_msg": f"è·å–ç¼“å­˜çŠ¶æ€å¼‚å¸¸: {e}",
            "data": None
        }


@app.post("/api/cache/preload",
          operation_id="preload_cache_data",
          summary="é¢„åŠ è½½ç¼“å­˜æ•°æ®")
async def preload_cache_data(request: CachePreloadRequest) -> Dict[str, Any]:
    """é¢„åŠ è½½ç¼“å­˜æ•°æ®"""
    start_time = time.time()
    
    try:
        if not cache_manager:
            return {
                "ret_code": -1,
                "ret_msg": "ç¼“å­˜ç®¡ç†å™¨æœªåˆå§‹åŒ–",
                "data": None
            }
        
        # æ‰§è¡Œé¢„åŠ è½½
        await cache_manager.preload_data(request.symbols, request.days)
        
        execution_time = time.time() - start_time
        
        response_data = CacheOperationResponse(
            success=True,
            message=f"æˆåŠŸé¢„åŠ è½½ {len(request.symbols)} åªè‚¡ç¥¨ {request.days} å¤©çš„æ•°æ®",
            affected_items=len(request.symbols),
            execution_time=execution_time
        )
        
        return {
            "ret_code": 0,
            "ret_msg": "ç¼“å­˜é¢„åŠ è½½å®Œæˆ",
            "data": response_data,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.exception(f"ç¼“å­˜é¢„åŠ è½½å¤±è´¥: {e}")
        return {
            "ret_code": -1,
            "ret_msg": f"ç¼“å­˜é¢„åŠ è½½å¼‚å¸¸: {e}",
            "data": None
        }


@app.delete("/api/cache/clear",
           operation_id="clear_cache_data",
           summary="æ¸…ç†ç¼“å­˜æ•°æ®")
async def clear_cache_data(request: CacheClearRequest) -> Dict[str, Any]:
    """æ¸…ç†ç¼“å­˜æ•°æ®"""
    start_time = time.time()
    
    try:
        if not cache_manager:
            return {
                "ret_code": -1,
                "ret_msg": "ç¼“å­˜ç®¡ç†å™¨æœªåˆå§‹åŒ–",
                "data": None
            }
        
        # æ‰§è¡Œæ¸…ç†
        await cache_manager.clear_cache(request.cache_type.value)
        
        execution_time = time.time() - start_time
        
        response_data = CacheOperationResponse(
            success=True,
            message=f"æˆåŠŸæ¸…ç† {request.cache_type.value} ç¼“å­˜",
            execution_time=execution_time
        )
        
        return {
            "ret_code": 0,
            "ret_msg": "ç¼“å­˜æ¸…ç†å®Œæˆ",
            "data": response_data,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.exception(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        return {
            "ret_code": -1,
            "ret_msg": f"ç¼“å­˜æ¸…ç†å¼‚å¸¸: {e}",
            "data": None
        }


# ==================== åŸæœ‰æ¥å£ä¿æŒå…¼å®¹ ====================
@app.post("/api/quote/stock_basicinfo",
          operation_id="get_stock_basicinfo",
          summary="è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯",
          description="è·å–æŒ‡å®šå¸‚åœºå’Œè¯åˆ¸ç±»å‹çš„è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯åˆ—è¡¨")
async def get_stock_basicinfo(request: StockBasicInfoRequest) -> APIResponse:
    """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_stock_basicinfo(request)
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}", data=None)


@app.post("/api/quote/trading_days", 
          operation_id="get_trading_days",
          summary="è·å–äº¤æ˜“æ—¥",
          description="è·å–æŒ‡å®šå¸‚åœºåœ¨æŒ‡å®šæ—¶é—´æ®µå†…çš„äº¤æ˜“æ—¥åˆ—è¡¨")
async def get_trading_days(request: TradingDaysRequest) -> APIResponse:
    """è·å–äº¤æ˜“æ—¥ - æŸ¥è¯¢æŒ‡å®šæ—¶é—´æ®µå†…çš„äº¤æ˜“æ—¥"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_trading_days(request)
    except Exception as e:
        logger.error(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}", data=None)


@app.post("/api/quote/capital_flow", 
          operation_id="get_capital_flow",
          summary="è·å–èµ„é‡‘æµå‘",
          description="è·å–ä¸ªè‚¡èµ„é‡‘æµå‘æ•°æ®ï¼ŒåŒ…æ‹¬ä¸»åŠ›ã€å¤§å•ã€ä¸­å•ã€å°å•çš„å‡€æµå…¥æƒ…å†µ")
async def get_capital_flow(request: CapitalFlowRequest) -> APIResponse:
    """è·å–èµ„é‡‘æµå‘ - åˆ†æä¸»åŠ›èµ„é‡‘åŠ¨å‘å’Œæ•£æˆ·æƒ…ç»ª"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_capital_flow(request)
    except Exception as e:
        logger.error(f"è·å–èµ„é‡‘æµå‘å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–èµ„é‡‘æµå‘å¤±è´¥: {e}", data=None)


@app.post("/api/quote/capital_distribution", 
          operation_id="get_capital_distribution",
          summary="è·å–èµ„é‡‘åˆ†å¸ƒ",
          description="è·å–ä¸ªè‚¡å½“å‰èµ„é‡‘åˆ†å¸ƒæƒ…å†µï¼Œåˆ†æç‰¹å¤§å•ã€å¤§å•ã€ä¸­å•ã€å°å•çš„æµå…¥æµå‡ºå¯¹æ¯”")
async def get_capital_distribution(request: CapitalDistributionRequest) -> APIResponse:
    """è·å–èµ„é‡‘åˆ†å¸ƒ"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    
    try:
        return await futu_service.get_capital_distribution(request)
    except Exception as e:
        logger.error(f"è·å–èµ„é‡‘åˆ†å¸ƒå¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–èµ„é‡‘åˆ†å¸ƒå¼‚å¸¸: {str(e)}")


@app.post("/api/quote/rehab", 
          operation_id="get_rehab",
          summary="è·å–å¤æƒå› å­",
          description="è·å–è‚¡ç¥¨å¤æƒå› å­æ•°æ®ï¼ŒåŒ…æ‹¬æ‹†è‚¡ã€åˆè‚¡ã€é€è‚¡ã€è½¬å¢è‚¡ã€é…è‚¡ã€å¢å‘ç­‰å…¬å¸è¡Œä¸ºçš„å¤æƒä¿¡æ¯")
async def get_rehab(request: RehabRequest) -> APIResponse:
    """è·å–å¤æƒå› å­"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    
    try:
        return await futu_service.get_rehab(request)
    except Exception as e:
        logger.error(f"è·å–å¤æƒå› å­å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤æƒå› å­å¼‚å¸¸: {str(e)}")


@app.post("/api/market/stock_filter", 
          operation_id="get_stock_filter",
          summary="æ¡ä»¶é€‰è‚¡",
          description="åŸºäºå¤šç§æ¡ä»¶ç­›é€‰è‚¡ç¥¨ï¼Œæ”¯æŒä»·æ ¼ã€æˆäº¤é‡ã€æŠ€æœ¯æŒ‡æ ‡ç­‰å¤šç»´åº¦ç­›é€‰ï¼Œæ”¯æŒæ¿å—è¿‡æ»¤")
async def get_stock_filter(request: StockFilterRequest) -> APIResponse:
    """æ¡ä»¶é€‰è‚¡"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    
    try:
        return await futu_service.get_stock_filter(request)
    except Exception as e:
        logger.error(f"æ¡ä»¶é€‰è‚¡å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¡ä»¶é€‰è‚¡å¼‚å¸¸: {str(e)}")


@app.post("/api/market/plate_stock", 
          operation_id="get_plate_stock",
          summary="è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨",
          description="è·å–æŒ‡å®šæ¿å—å†…çš„æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨ï¼Œæ”¯æŒæŒ‰å­—æ®µæ’åº")
async def get_plate_stock(request: PlateStockRequest) -> APIResponse:
    """è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    
    try:
        return await futu_service.get_plate_stock(request)
    except Exception as e:
        logger.error(f"è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸: {str(e)}")


@app.post("/api/market/plate_list", 
          operation_id="get_plate_list",
          summary="è·å–æ¿å—åˆ—è¡¨",
          description="è·å–æŒ‡å®šå¸‚åœºçš„æ¿å—åˆ—è¡¨ï¼Œæ”¯æŒæŒ‰æ¿å—ç±»å‹è¿‡æ»¤ï¼ˆè¡Œä¸šã€æ¦‚å¿µã€åœ°åŸŸæ¿å—ï¼‰")
async def get_plate_list(request: PlateListRequest) -> APIResponse:
    """è·å–æ¿å—åˆ—è¡¨"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    
    try:
        return await futu_service.get_plate_list(request)
    except Exception as e:
        logger.error(f"è·å–æ¿å—åˆ—è¡¨å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¿å—åˆ—è¡¨å¼‚å¸¸: {str(e)}")


# ==================== äº¤æ˜“ç›¸å…³æ¥å£ ====================

@app.post("/api/trade/acc_info",
          operation_id="get_acc_info", 
          summary="æŸ¥è¯¢è´¦æˆ·èµ„é‡‘",
          description="æŸ¥è¯¢äº¤æ˜“ä¸šåŠ¡è´¦æˆ·çš„èµ„äº§å‡€å€¼ã€è¯åˆ¸å¸‚å€¼ã€ç°é‡‘ã€è´­ä¹°åŠ›ç­‰èµ„é‡‘æ•°æ®")
async def get_acc_info(request: AccInfoRequest) -> APIResponse:
    """æŸ¥è¯¢è´¦æˆ·èµ„é‡‘ - è·å–è´¦æˆ·æ€»èµ„äº§ã€ç°é‡‘ã€è´­ä¹°åŠ›ç­‰èµ„é‡‘ä¿¡æ¯"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    if not futu_service.has_trade_connection():
        raise HTTPException(status_code=503, detail="äº¤æ˜“æ¥å£æœªè¿æ¥æˆ–æœªè§£é”ï¼Œè¯·å…ˆåœ¨OpenDä¸­å¼€å¯äº¤æ˜“åŠŸèƒ½")
    
    try:
        return await futu_service.get_acc_info(request)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢è´¦æˆ·èµ„é‡‘å¼‚å¸¸: {str(e)}")
        
        # å¦‚æœæ˜¯äº¤æ˜“è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "äº¤æ˜“æœªè¿æ¥" in error_msg:
            error_msg = "äº¤æ˜“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·ç¡®ä¿å¯Œé€”OpenDå·²å¯åŠ¨ä¸”æ”¯æŒäº¤æ˜“åŠŸèƒ½"
        elif "å¯†ç " in error_msg.lower() or "unlock" in error_msg.lower():
            error_msg = "äº¤æ˜“å¯†ç éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯†ç é…ç½®"
        elif "è´¦æˆ·" in error_msg:
            error_msg = "è´¦æˆ·ä¿¡æ¯æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥è´¦æˆ·IDæˆ–è´¦æˆ·ç´¢å¼•"
        
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢è´¦æˆ·èµ„é‡‘å¤±è´¥: {error_msg}")


@app.post("/api/trade/position_list",
          operation_id="get_position_list",
          summary="æŸ¥è¯¢æŒä»“åˆ—è¡¨", 
          description="æŸ¥è¯¢äº¤æ˜“ä¸šåŠ¡è´¦æˆ·çš„æŒä»“åˆ—è¡¨ï¼Œæ”¯æŒä»£ç è¿‡æ»¤ã€å¸‚åœºè¿‡æ»¤ã€ç›ˆäºæ¯”ä¾‹è¿‡æ»¤ç­‰å¤šç§ç­›é€‰æ¡ä»¶")
async def get_position_list(request: PositionListRequest) -> APIResponse:
    """æŸ¥è¯¢æŒä»“åˆ—è¡¨ - è·å–è´¦æˆ·æ‰€æœ‰æŒä»“ä¿¡æ¯ï¼ŒåŒ…å«ç›ˆäºåˆ†æå’Œå¸‚åœºåˆ†å¸ƒ"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    if not futu_service.has_trade_connection():
        raise HTTPException(status_code=503, detail="äº¤æ˜“æ¥å£æœªè¿æ¥æˆ–æœªè§£é”ï¼Œè¯·å…ˆåœ¨OpenDä¸­å¼€å¯äº¤æ˜“åŠŸèƒ½")
    
    try:
        return await futu_service.get_position_list(request)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æŒä»“åˆ—è¡¨å¼‚å¸¸: {str(e)}")
        
        # å¦‚æœæ˜¯äº¤æ˜“è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "äº¤æ˜“æœªè¿æ¥" in error_msg:
            error_msg = "äº¤æ˜“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·ç¡®ä¿å¯Œé€”OpenDå·²å¯åŠ¨ä¸”æ”¯æŒäº¤æ˜“åŠŸèƒ½"
        elif "å¯†ç " in error_msg.lower() or "unlock" in error_msg.lower():
            error_msg = "äº¤æ˜“å¯†ç éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯†ç é…ç½®"
        elif "è´¦æˆ·" in error_msg:
            error_msg = "è´¦æˆ·ä¿¡æ¯æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥è´¦æˆ·IDæˆ–è´¦æˆ·ç´¢å¼•"
        elif "æŒä»“" in error_msg:
            error_msg = "æŒä»“æ•°æ®è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è´¦æˆ·æ˜¯å¦æœ‰æŒä»“æˆ–ç½‘ç»œè¿æ¥"
        
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢æŒä»“åˆ—è¡¨å¤±è´¥: {error_msg}")


@app.post("/api/trade/history_deal_list",
          operation_id="get_history_deal_list",
          summary="æŸ¥è¯¢å†å²æˆäº¤", 
          description="æŸ¥è¯¢äº¤æ˜“ä¸šåŠ¡è´¦æˆ·çš„å†å²æˆäº¤åˆ—è¡¨ï¼Œæ”¯æŒä»£ç è¿‡æ»¤ã€å¸‚åœºè¿‡æ»¤ã€æ—¶é—´èŒƒå›´è¿‡æ»¤ã€‚æ³¨æ„ï¼šä»…æ”¯æŒçœŸå®ç¯å¢ƒ")
async def get_history_deal_list(request: HistoryDealListRequest) -> APIResponse:
    """æŸ¥è¯¢å†å²æˆäº¤ - è·å–è´¦æˆ·å†å²æˆäº¤è®°å½•ï¼ŒåŒ…å«ä¹°å–åˆ†æå’Œè´¹ç”¨ç»Ÿè®¡"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    if not futu_service.has_trade_connection():
        raise HTTPException(status_code=503, detail="äº¤æ˜“æ¥å£æœªè¿æ¥æˆ–æœªè§£é”ï¼Œè¯·å…ˆåœ¨OpenDä¸­å¼€å¯äº¤æ˜“åŠŸèƒ½")
    
    try:
        return await futu_service.get_history_deal_list(request)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å†å²æˆäº¤å¼‚å¸¸: {str(e)}")
        
        # å¦‚æœæ˜¯äº¤æ˜“è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "äº¤æ˜“æœªè¿æ¥" in error_msg:
            error_msg = "äº¤æ˜“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·ç¡®ä¿å¯Œé€”OpenDå·²å¯åŠ¨ä¸”æ”¯æŒäº¤æ˜“åŠŸèƒ½"
        elif "å¯†ç " in error_msg.lower() or "unlock" in error_msg.lower():
            error_msg = "äº¤æ˜“å¯†ç éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯†ç é…ç½®"
        elif "è´¦æˆ·" in error_msg:
            error_msg = "è´¦æˆ·ä¿¡æ¯æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥è´¦æˆ·IDæˆ–è´¦æˆ·ç´¢å¼•"
        elif "å†å²æˆäº¤" in error_msg or "æˆäº¤" in error_msg:
            error_msg = "å†å²æˆäº¤æ•°æ®è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´æˆ–è´¦æˆ·æƒé™"
        elif "æ¨¡æ‹Ÿ" in error_msg:
            error_msg = "å†å²æˆäº¤æŸ¥è¯¢ä»…æ”¯æŒçœŸå®ç¯å¢ƒï¼Œä¸æ”¯æŒæ¨¡æ‹Ÿç¯å¢ƒ"
        
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å†å²æˆäº¤å¤±è´¥: {error_msg}")


@app.post("/api/trade/deal_list",
          operation_id="get_deal_list",
          summary="æŸ¥è¯¢å½“æ—¥æˆäº¤", 
          description="æŸ¥è¯¢äº¤æ˜“ä¸šåŠ¡è´¦æˆ·çš„å½“æ—¥æˆäº¤åˆ—è¡¨ï¼Œæ”¯æŒä»£ç è¿‡æ»¤ã€å¸‚åœºè¿‡æ»¤ï¼ŒåŒ…å«å®æ—¶æˆäº¤ç»Ÿè®¡")
async def get_deal_list(request: DealListRequest) -> APIResponse:
    """æŸ¥è¯¢å½“æ—¥æˆäº¤ - è·å–è´¦æˆ·å½“æ—¥æˆäº¤è®°å½•ï¼ŒåŒ…å«ä¹°å–åˆ†æå’Œæ—¶é—´åˆ†å¸ƒ"""
    if not _server_ready or not futu_service:
        raise HTTPException(status_code=503, detail="æœåŠ¡æœªå°±ç»ª")
    if not futu_service.has_trade_connection():
        raise HTTPException(status_code=503, detail="äº¤æ˜“æ¥å£æœªè¿æ¥æˆ–æœªè§£é”ï¼Œè¯·å…ˆåœ¨OpenDä¸­å¼€å¯äº¤æ˜“åŠŸèƒ½")
    
    try:
        return await futu_service.get_deal_list(request)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å½“æ—¥æˆäº¤å¼‚å¸¸: {str(e)}")
        
        # å¦‚æœæ˜¯äº¤æ˜“è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "äº¤æ˜“æœªè¿æ¥" in error_msg:
            error_msg = "äº¤æ˜“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·ç¡®ä¿å¯Œé€”OpenDå·²å¯åŠ¨ä¸”æ”¯æŒäº¤æ˜“åŠŸèƒ½"
        elif "å¯†ç " in error_msg.lower() or "unlock" in error_msg.lower():
            error_msg = "äº¤æ˜“å¯†ç éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯†ç é…ç½®"
        elif "è´¦æˆ·" in error_msg:
            error_msg = "è´¦æˆ·ä¿¡æ¯æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥è´¦æˆ·IDæˆ–è´¦æˆ·ç´¢å¼•"
        elif "å½“æ—¥æˆäº¤" in error_msg or "æˆäº¤" in error_msg:
            error_msg = "å½“æ—¥æˆäº¤æ•°æ®è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è´¦æˆ·æƒé™æˆ–ç½‘ç»œè¿æ¥"
        
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å½“æ—¥æˆäº¤å¤±è´¥: {error_msg}")


@app.post("/api/trade/history_order_list",
          operation_id="get_history_order_list",
          summary="è·å–å†å²è®¢å•åˆ—è¡¨",
          description="æŸ¥è¯¢æŒ‡å®šæ—¶é—´æ®µå†…çš„å†å²è®¢å•è®°å½•ï¼Œæ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶")
async def get_history_order_list(request: HistoryOrderListRequest) -> APIResponse:
    """è·å–å†å²è®¢å•åˆ—è¡¨"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not futu_service or not futu_service.has_trade_connection():
        return APIResponse(ret_code=-1, ret_msg="äº¤æ˜“æ¥å£æœªè¿æ¥æˆ–æœªè§£é”ï¼Œè¯·æ£€æŸ¥OpenDçŠ¶æ€åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_history_order_list(request)
    except Exception as e:
        logger.error(f"è·å–å†å²è®¢å•åˆ—è¡¨å¤±è´¥: {e}")
        
        # å¦‚æœæ˜¯äº¤æ˜“è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "äº¤æ˜“æœªè¿æ¥" in error_msg:
            error_msg = "äº¤æ˜“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·ç¡®ä¿å¯Œé€”OpenDå·²å¯åŠ¨ä¸”æ”¯æŒäº¤æ˜“åŠŸèƒ½"
        elif "å¯†ç " in error_msg.lower() or "unlock" in error_msg.lower():
            error_msg = "äº¤æ˜“å¯†ç éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯†ç é…ç½®"
        elif "è´¦æˆ·" in error_msg:
            error_msg = "è´¦æˆ·ä¿¡æ¯æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥è´¦æˆ·IDæˆ–è´¦æˆ·ç´¢å¼•"
        
        return APIResponse(ret_code=-1, ret_msg=f"è·å–å†å²è®¢å•åˆ—è¡¨å¤±è´¥: {error_msg}", data=None)


@app.post("/api/trade/order_fee_query",
          operation_id="get_order_fee_query",
          summary="æŸ¥è¯¢è®¢å•è´¹ç”¨",
          description="æŸ¥è¯¢æŒ‡å®šè®¢å•çš„è¯¦ç»†è´¹ç”¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½£é‡‘ã€å°èŠ±ç¨ç­‰")
async def get_order_fee_query(request: OrderFeeQueryRequest) -> APIResponse:
    """æŸ¥è¯¢è®¢å•è´¹ç”¨"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not futu_service or not futu_service.has_trade_connection():
        return APIResponse(ret_code=-1, ret_msg="äº¤æ˜“æ¥å£æœªè¿æ¥æˆ–æœªè§£é”ï¼Œè¯·æ£€æŸ¥OpenDçŠ¶æ€åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_order_fee_query(request)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢è®¢å•è´¹ç”¨å¤±è´¥: {e}")
        
        # å¦‚æœæ˜¯äº¤æ˜“è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "äº¤æ˜“æœªè¿æ¥" in error_msg:
            error_msg = "äº¤æ˜“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·ç¡®ä¿å¯Œé€”OpenDå·²å¯åŠ¨ä¸”æ”¯æŒäº¤æ˜“åŠŸèƒ½"
        elif "å¯†ç " in error_msg.lower() or "unlock" in error_msg.lower():
            error_msg = "äº¤æ˜“å¯†ç éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯†ç é…ç½®"
        elif "è®¢å•" in error_msg:
            error_msg = "è®¢å•ä¿¡æ¯æœ‰è¯¯ï¼Œè¯·æ£€æŸ¥è®¢å•IDæ˜¯å¦æ­£ç¡®"
        
        return APIResponse(ret_code=-1, ret_msg=f"æŸ¥è¯¢è®¢å•è´¹ç”¨å¤±è´¥: {error_msg}", data=None)


@app.post("/api/trade/trade_history",
          operation_id="get_trade_history",
          summary="è·å–äº¤æ˜“å†å²",
          description="è·å–å†å²æˆäº¤è®°å½•ï¼ˆhistory_deal_listçš„åˆ«åæ¥å£ï¼‰")
async def get_trade_history(request: HistoryDealListRequest) -> APIResponse:
    """è·å–äº¤æ˜“å†å²ï¼ˆå†å²æˆäº¤çš„åˆ«åï¼‰"""
    return await get_history_deal_list(request)


# ==================== MCPå®æ—¶çœ‹æ¿æ¥å£ ====================
@app.post("/api/dashboard/session",
          operation_id="create_dashboard_session",
          summary="åˆ›å»ºè‚¡ç¥¨å®æ—¶çœ‹æ¿ä¼šè¯",
          description="ç”Ÿæˆä¸€ä¸ªå¯åˆ†äº«çš„ Web çœ‹æ¿ URLï¼Œç”¨äºå®æ—¶æŸ¥çœ‹æŒ‡å®šè‚¡ç¥¨çš„è¡Œæƒ…/èµ„è®¯/ç­–ç•¥",
          tags=["dashboard"])
async def create_dashboard_session(request: DashboardSessionRequest, http_request: Request) -> DashboardSessionResponse:
    if not _server_ready:
        raise HTTPException(status_code=503, detail="æœåŠ¡å°šæœªå°±ç»ª")
    session_id = secrets.token_urlsafe(10)
    async with _dashboard_lock:
        existing_session_id = None
        for sid, info in dashboard_sessions.items():
            if info.get("code") == request.code:
                existing_session_id = sid
                if request.nickname:
                    info["nickname"] = request.nickname
                break
        if existing_session_id:
            dashboard_sessions[existing_session_id].setdefault("history", [])
            session_id = existing_session_id
        else:
            dashboard_sessions[session_id] = {
                "code": request.code,
                "nickname": request.nickname,
                "created_at": datetime.now().isoformat(),
                "history": []
            }
    await _persist_dashboard_sessions()
    base_url = settings.dashboard_base_url.rstrip("/")
    url = f"{base_url}/web/dashboard?session={session_id}"
    if dashboard_stream_manager:
        dashboard_stream_manager.register_session(session_id, request.code)
        await dashboard_stream_manager.ensure_code_subscription(code=request.code)
        dashboard_stream_manager.start_guard()
    logger.info(f"[dashboard.session.create] session={session_id} code={request.code} nickname={request.nickname} reused={existing_session_id is not None}")
    return DashboardSessionResponse(session_id=session_id, url=url)


@app.get("/api/dashboard/sessions",
         operation_id="list_dashboard_sessions",
         summary="åˆ—å‡ºæ‰€æœ‰çœ‹æ¿ä¼šè¯",
         tags=["dashboard"])
async def list_dashboard_sessions() -> Dict[str, Any]:
    start = time.time()
    removed_duplicates: List[Tuple[str, str]] = []
    async with _dashboard_lock:
        removed_duplicates = _remove_duplicate_sessions_locked()
        session_items = [
            DashboardSessionItem(
                session_id=session_id,
                code=info.get("code"),
                nickname=info.get("nickname"),
                created_at=info.get("created_at"),
            )
            for session_id, info in dashboard_sessions.items()
        ]
    if removed_duplicates:
        await _persist_dashboard_sessions()
        if dashboard_stream_manager:
            for session_id, code in removed_duplicates:
                await dashboard_stream_manager.unregister_session(session_id)
                await dashboard_stream_manager.force_detach_session(session_id)
        logger.warning(f"[dashboard.sessions.dedup] removed={len(removed_duplicates)} codes={[c for _, c in removed_duplicates]}")
    quota = None
    if futu_service:
        summary = await futu_service.get_subscription_summary()
        if summary.ret_code == 0 and isinstance(summary.data, dict):
            raw_quota = summary.data
            total_used = raw_quota.get("total_used") or raw_quota.get("totalUsed") or raw_quota.get("used") or 0
            remain = raw_quota.get("remain") or raw_quota.get("remainQuota") or raw_quota.get("remain_count") or 0
            own_used = raw_quota.get("own_used") or raw_quota.get("ownUsed") or raw_quota.get("self_used") or 0
            quota = {
                "total_used": total_used,
                "remain": remain,
                "own_used": own_used,
                "raw": raw_quota
            }
        # è·å–è¡Œæƒ…å¿«ç…§ + æœ€æ–°ç­–ç•¥
        for item in session_items:
            quote = await _fetch_quote_snapshot(item.code)
            if quote:
                item.quote = quote
            if recommendation_storage:
                rec_req = RecommendationQueryRequest(code=item.code, limit=1)
                recs = await asyncio.to_thread(recommendation_storage.get_recommendations, rec_req.dict())
                if recs:
                    latest = recs[0]
                    item.strategy = latest.get("action")
                    item.last_signal_time = latest.get("created_at")
    cost = time.time() - start
    logger.info(f"[dashboard.sessions.list] total={len(session_items)} quota={'y' if quota else 'n'} cost={cost:.3f}s")
    return {"sessions": [item.model_dump() for item in session_items], "quota": quota}


@app.delete("/api/dashboard/session/{session_id}",
            operation_id="delete_dashboard_session",
            summary="åˆ é™¤çœ‹æ¿ä¼šè¯",
            tags=["dashboard"])
async def delete_dashboard_session(session_id: str) -> Dict[str, Any]:
    async with _dashboard_lock:
        info = dashboard_sessions.pop(session_id, None)
    if not info:
        raise HTTPException(status_code=404, detail="session not found")
    await _persist_dashboard_sessions()
    if dashboard_stream_manager:
        await dashboard_stream_manager.unregister_session(session_id)
        await dashboard_stream_manager.force_detach_session(session_id)
    return {"deleted": True, "session_id": session_id, "code": info.get("code")}


DEFAULT_DASHBOARD_MODULES = {"core", "signals", "recommendations", "capital", "kline"}


@app.get("/api/dashboard/bootstrap",
         operation_id="get_dashboard_bootstrap",
         summary="è·å–çœ‹æ¿åˆå§‹æ•°æ®",
         tags=["dashboard"])
async def get_dashboard_bootstrap(session: str, modules: Optional[str] = None) -> Dict[str, Any]:
    data = _ensure_dashboard_session(session)
    code = data["code"]
    module_set = (
        {m.strip().lower() for m in modules.split(",") if m.strip()} if modules else set(DEFAULT_DASHBOARD_MODULES)
    )
    module_set &= DEFAULT_DASHBOARD_MODULES
    if not module_set:
        module_set = set(DEFAULT_DASHBOARD_MODULES)

    start = time.time()
    logger.info(f"[dashboard.bootstrap] session={session} code={code} modules={','.join(sorted(module_set))}")

    def _noop_task(result=None):
        return asyncio.create_task(asyncio.sleep(0, result=result))

    tasks: Dict[str, asyncio.Task] = {}
    if "core" in module_set:
        tasks["quote"] = asyncio.create_task(_fetch_quote_snapshot(code))
        tasks["holding"] = asyncio.create_task(_fetch_holding_snapshot(code))
    if "signals" in module_set:
        tasks["signals"] = asyncio.create_task(_fetch_news_signals(code, 12, refresh=False))
    if "recommendations" in module_set:
        tasks["recommendations"] = asyncio.create_task(_fetch_recommendations_snapshot(code))
    if "capital" in module_set:
        if futu_service:
            start_date = (datetime.now() - timedelta(days=5)).strftime("%Y-%m-%d")
            tasks["capital_flow"] = asyncio.create_task(
                futu_service.get_capital_flow(
                    CapitalFlowRequest(code=code, period_type=PeriodType.DAY, start=start_date)
                )
            )
            tasks["capital_distribution"] = asyncio.create_task(
                futu_service.get_capital_distribution(CapitalDistributionRequest(code=code))
            )
        else:
            tasks["capital_flow"] = _noop_task()
            tasks["capital_distribution"] = _noop_task()
    if "kline" in module_set:
        if futu_service:
            tasks["history_kline"] = asyncio.create_task(
                futu_service.get_history_kline(HistoryKLineRequest(code=code, ktype=KLType.K_1M, max_count=240))
            )
            tasks["current_kline"] = asyncio.create_task(
                futu_service.get_current_kline(
                    CurrentKLineRequest(code=code, num=240, ktype=KLType.K_1M, autype=AuType.QFQ)
                )
            )
        else:
            tasks["history_kline"] = _noop_task()
            tasks["current_kline"] = _noop_task()

    if tasks:
        await asyncio.gather(*tasks.values(), return_exceptions=True)

    def _task_result(key: str):
        task = tasks.get(key)
        if not task:
            return None
        if task.cancelled():
            logger.warning(f"[dashboard.module.cancelled] session={session} module={key}")
            return None
        exc = task.exception()
        if exc:
            logger.warning(f"[dashboard.module.error] session={session} module={key} error={exc}")
            return None
        try:
            return task.result()
        except Exception as task_exc:
            logger.warning(f"[dashboard.module.result_error] session={session} module={key} error={task_exc}")
            return None

    history = data.get("history", [])
    bootstrap: Dict[str, Any] = {
        "code": code,
        "session": {
            "session_id": session,
            "nickname": data.get("nickname"),
            "created_at": data.get("created_at")
        },
        "history": history[-60:]
    }

    if "core" in module_set:
        quote = _task_result("quote")
        holding = _task_result("holding")
        session_window = _get_session_window(code)
        if quote:
            bootstrap["quote"] = quote
            if quote.get("prev_close") is not None:
                session_window["previous_close"] = quote.get("prev_close")
        if holding is not None:
            bootstrap["holding"] = holding
        bootstrap["session_window"] = session_window

    if "signals" in module_set and "signals" in tasks:
        signals_result = _task_result("signals")
        if signals_result is not None:
            bootstrap["signals"] = signals_result

    if "recommendations" in module_set and "recommendations" in tasks:
        rec_result = _task_result("recommendations")
        if rec_result is not None:
            bootstrap["recommendations"] = rec_result

    if "capital" in module_set:
        flow_resp = _task_result("capital_flow")
        distribution_resp = _task_result("capital_distribution")
        if flow_resp:
            if isinstance(flow_resp, APIResponse) and flow_resp.ret_code == 0:
                bootstrap["capital_flow"] = flow_resp.data
        if distribution_resp:
            if isinstance(distribution_resp, APIResponse) and distribution_resp.ret_code == 0:
                bootstrap["capital_distribution"] = distribution_resp.data

    if "kline" in module_set:
        history_result = _task_result("history_kline")
        current_result = _task_result("current_kline")
        history_data = (
            history_result.data.get("kline_data")
            if isinstance(history_result, APIResponse)
            and history_result.ret_code == 0
            and history_result.data
            else None
        )
        current_data = (
            current_result.data.get("kline_data")
            if isinstance(current_result, APIResponse)
            and current_result.ret_code == 0
            and current_result.data
            else None
        )
        merged_kline = _merge_kline_records(history_data, current_data)
        if minute_kline_storage and merged_kline:
            try:
                minute_kline_storage.save_batch(code, merged_kline)
                minute_kline_storage.delete_older_than(code, keep_limit=2880)
                stored_recent = minute_kline_storage.fetch_recent(code, limit=720)
                merged_kline = _merge_kline_records(stored_recent, merged_kline)
            except Exception as exc:
                logger.warning(f"åˆ†é’ŸKçº¿å­˜å‚¨å¤±è´¥: {exc}")
        if merged_kline:
            bootstrap["history_kline"] = merged_kline

    logger.info(
        f"[dashboard.bootstrap.done] session={session} code={code} modules={','.join(sorted(module_set))} cost={time.time()-start:.3f}s"
    )
    return bootstrap


@app.get("/web/api/stream/{session_id}", include_in_schema=False)
async def dashboard_stream(session_id: str):
    session = _ensure_dashboard_session(session_id)
    code = session["code"]
    
    if not dashboard_stream_manager:
        raise HTTPException(status_code=503, detail="DashboardæµåŠŸèƒ½ä¸å¯ç”¨")
    
    queue = await dashboard_stream_manager.attach_session(session_id, code)
    
    async def event_generator():
        try:
            logger.info(f"[dashboard.stream.attach] session={session_id} code={code}")
            while True:
                payload = await queue.get()
                history = session.setdefault("history", [])
                quote = payload.get("quote")
                if quote:
                    history.append({"ts": payload.get("timestamp"), "price": quote.get("price")})
                    if len(history) > 180:
                        history.pop(0)
                try:
                    chunk = json.dumps(payload, ensure_ascii=False)
                    yield f"data: {chunk}\\n\\n"
                except Exception as exc:
                    logger.debug(f"dashboard stream encode error: {exc}")
        except asyncio.CancelledError:
            logger.debug("dashboard stream cancelled")
        except Exception as exc:
            logger.warning(f"dashboard stream error: {exc}")
        finally:
            await dashboard_stream_manager.release_stream(session_id)
            logger.info(f"[dashboard.stream.detach] session={session_id} code={code}")
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")


# ==================== ç­–ç•¥å»ºè®®è®°å½•æ¥å£ ====================
@app.post("/api/recommendations",
          operation_id="save_recommendation",
          summary="ä¿å­˜ç­–ç•¥å»ºè®®",
          description="è®°å½•é’ˆå¯¹æŸæ ‡çš„çš„æ“ä½œå»ºè®®ã€ä¿¡å¿ƒã€ä¾æ®ç­‰ä¿¡æ¯ï¼Œç”¨äºåç»­å¤ç›˜",
          tags=["recommendation"])
async def save_recommendation(request: RecommendationWriteRequest) -> APIResponse:
    """ä¿å­˜ç­–ç•¥å»ºè®®"""
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    
    try:
        if request.status == "running":
            has_other = await asyncio.to_thread(recommendation_storage.has_running_strategy, request.code)
            if has_other:
                return APIResponse(ret_code=-1, ret_msg="è¯¥è‚¡ç¥¨å·²æœ‰æ‰§è¡Œä¸­çš„ç­–ç•¥", data=None)
        saved = await asyncio.to_thread(recommendation_storage.save_recommendation, request.dict())
        if strategy_monitor and request.status == "running" and saved.get("id"):
            full = await asyncio.to_thread(recommendation_storage.get_recommendation, saved["id"])
            if full:
                await strategy_monitor.register_strategy(full)
        return APIResponse(ret_code=0, ret_msg="ç­–ç•¥å»ºè®®ä¿å­˜æˆåŠŸ", data=saved)
    except Exception as e:
        logger.error(f"ä¿å­˜ç­–ç•¥å»ºè®®å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"ç­–ç•¥å»ºè®®ä¿å­˜å¤±è´¥: {e}", data=None)


@app.post("/api/recommendations/query",
          operation_id="get_recommendations",
          summary="æŸ¥è¯¢ç­–ç•¥å»ºè®®",
          description="æŒ‰è‚¡ç¥¨ä»£ç ã€æ“ä½œç±»å‹ã€é‡‡çº³çŠ¶æ€ã€æ—¶é—´åŒºé—´ç­‰æ¡ä»¶æ£€ç´¢å†å²ç­–ç•¥å»ºè®®",
          tags=["recommendation"])
async def get_recommendations(request: RecommendationQueryRequest) -> APIResponse:
    """æŸ¥è¯¢ç­–ç•¥å»ºè®®"""
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    
    try:
        items = await asyncio.to_thread(recommendation_storage.get_recommendations, request.dict())
        return APIResponse(ret_code=0, ret_msg="æŸ¥è¯¢æˆåŠŸ", data={"items": items, "count": len(items)})
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ç­–ç•¥å»ºè®®å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"æŸ¥è¯¢ç­–ç•¥å»ºè®®å¤±è´¥: {e}", data=None)


@app.get("/api/recommendations/{rec_id}",
         operation_id="get_recommendation_detail",
         summary="æŸ¥è¯¢å•æ¡ç­–ç•¥è¯¦æƒ…",
         tags=["recommendation"])
async def get_recommendation_detail(rec_id: int) -> APIResponse:
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    try:
        item = await asyncio.to_thread(recommendation_storage.get_recommendation, rec_id)
        if not item:
            raise HTTPException(status_code=404, detail="ç­–ç•¥ä¸å­˜åœ¨")
        return APIResponse(ret_code=0, ret_msg="æŸ¥è¯¢æˆåŠŸ", data=item)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ç­–ç•¥è¯¦æƒ…å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"æŸ¥è¯¢ç­–ç•¥è¯¦æƒ…å¤±è´¥: {e}", data=None)


@app.get("/api/recommendations/{rec_id}/evaluations",
         operation_id="get_recommendation_evaluations",
         summary="æŸ¥è¯¢ç­–ç•¥è¯„ä¼°å†å²",
         tags=["recommendation"])
async def get_recommendation_evaluations(rec_id: int) -> APIResponse:
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    try:
        history = await asyncio.to_thread(recommendation_storage.get_evaluations, rec_id)
        return APIResponse(ret_code=0, ret_msg="æŸ¥è¯¢æˆåŠŸ", data={"items": history, "count": len(history)})
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ç­–ç•¥è¯„ä¼°å†å²å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"æŸ¥è¯¢ç­–ç•¥è¯„ä¼°å†å²å¤±è´¥: {e}", data=None)


@app.get("/api/recommendations/{rec_id}/alerts",
         operation_id="get_recommendation_alerts",
         summary="æŸ¥è¯¢ç­–ç•¥ç›¯ç›˜å‘Šè­¦",
         tags=["recommendation"])
async def get_recommendation_alerts(rec_id: int, limit: int = 50) -> APIResponse:
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    try:
        alerts = await asyncio.to_thread(recommendation_storage.get_alerts, rec_id, limit)
        return APIResponse(ret_code=0, ret_msg="æŸ¥è¯¢æˆåŠŸ", data={"items": alerts, "count": len(alerts)})
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ç­–ç•¥å‘Šè­¦å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"æŸ¥è¯¢ç­–ç•¥å‘Šè­¦å¤±è´¥: {e}", data=None)


@app.patch("/api/recommendations/{rec_id}",
           operation_id="update_recommendation",
           summary="æ›´æ–°ç­–ç•¥å»ºè®®çŠ¶æ€",
           tags=["recommendation"])
async def update_recommendation(rec_id: int, request: RecommendationUpdateRequest) -> APIResponse:
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    try:
        existing = await asyncio.to_thread(recommendation_storage.get_recommendation, rec_id)
        if not existing:
            raise HTTPException(status_code=404, detail="ç­–ç•¥ä¸å­˜åœ¨")
        payload = request.dict(exclude_unset=True)
        new_status = payload.get("status")
        if new_status == "running":
            has_other = await asyncio.to_thread(
                recommendation_storage.has_running_strategy,
                existing["code"],
                rec_id,
            )
            if has_other:
                return APIResponse(ret_code=-1, ret_msg="è¯¥è‚¡ç¥¨å·²æœ‰æ‰§è¡Œä¸­çš„ç­–ç•¥", data=None)
        updated = await asyncio.to_thread(recommendation_storage.update_recommendation, rec_id, payload)
        if not updated:
            raise HTTPException(status_code=404, detail="ç­–ç•¥ä¸å­˜åœ¨")
        if strategy_monitor:
            if payload.get("status"):
                if payload["status"] == "running":
                    await strategy_monitor.register_strategy(updated)
                else:
                    await strategy_monitor.unregister_strategy(rec_id)
            elif updated.get("status") == "running" and any(
                key in payload for key in ["monitor_config", "entry_price", "target_price", "stop_loss"]
            ):
                await strategy_monitor.register_strategy(updated)
        return APIResponse(ret_code=0, ret_msg="æ›´æ–°æˆåŠŸ", data=updated)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°ç­–ç•¥å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"æ›´æ–°ç­–ç•¥å¤±è´¥: {e}", data=None)


@app.post("/api/recommendations/{rec_id}/reevaluate",
          operation_id="reevaluate_recommendation",
          summary="é‡æ–°è¯„ä¼°ç­–ç•¥",
          tags=["recommendation"])
async def reevaluate_recommendation(rec_id: int, request: RecommendationReevaluateRequest) -> APIResponse:
    if recommendation_storage is None:
        return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥å»ºè®®å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–", data=None)
    if not _server_ready or not multi_model_service:
        return APIResponse(ret_code=-1, ret_msg="å¤šæ¨¡å‹åˆ†ææœåŠ¡ä¸å¯ç”¨", data=None)
    try:
        item = await asyncio.to_thread(recommendation_storage.get_recommendation, rec_id)
        if not item:
            raise HTTPException(status_code=404, detail="ç­–ç•¥ä¸å­˜åœ¨")
        code = item.get("code")
        if not code:
            return APIResponse(ret_code=-1, ret_msg="ç­–ç•¥ç¼ºå°‘è‚¡ç¥¨ä»£ç ", data=None)
        snapshot = await get_analysis_snapshot(AnalysisSnapshotRequest(code=code))
        context_text = _build_analysis_context_text(snapshot)
        result = await multi_model_service.run_analysis(
            code=code,
            models=request.models,
            judge_model=request.judge_model,
            context_text=context_text,
            context_snapshot=snapshot,
            question=request.question,
        )
        quote = result.get("context_snapshot", {}).get("quote") or snapshot.get("quote") or {}
        current_price = quote.get("price") or quote.get("cur_price")
        entry_price = item.get("entry_price")
        pnl_pct = None
        try:
            if entry_price is not None and current_price is not None:
                entry_val = float(entry_price)
                if entry_val != 0:
                    pnl_pct = (float(current_price) - entry_val) / entry_val
        except (TypeError, ValueError):
            pnl_pct = None
        summary = (
            result.get("judge", {}).get("result", {}).get("summary")
            or "å¤šæ¨¡å‹å·²å®Œæˆæœ€æ–°è¯„ä¼°"
        )
        now_iso = datetime.now(timezone.utc).isoformat()
        update_payload: Dict[str, Any] = {
            "eval_status": "completed",
            "eval_summary": summary,
            "eval_generated_at": now_iso,
            "eval_pnl_pct": pnl_pct,
            "eval_detail": {"analysis": result},
        }
        updated = await asyncio.to_thread(
            recommendation_storage.update_recommendation, rec_id, update_payload
        )
        history_record = await asyncio.to_thread(
            recommendation_storage.add_evaluation_record,
            rec_id,
            summary=summary,
            pnl=pnl_pct,
            detail={"analysis": result} if result else None,
            models=request.models,
            judge_model=request.judge_model,
            created_at=now_iso,
        )
        return APIResponse(
            ret_code=0,
            ret_msg="é‡æ–°è¯„ä¼°å®Œæˆ",
            data={
                "analysis": result,
                "evaluation": {
                    "summary": summary,
                    "generated_at": now_iso,
                    "pnl_pct": pnl_pct,
                },
                "history_record": history_record,
                "updated": updated,
            },
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é‡æ–°è¯„ä¼°ç­–ç•¥å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"é‡æ–°è¯„ä¼°ç­–ç•¥å¤±è´¥: {e}", data=None)


# ==================== åŸºæœ¬é¢æœç´¢æ¥å£ ====================

@app.post("/api/fundamental/search",
          operation_id="get_fundamental_search",
          summary="ğŸ” åŸºæœ¬é¢ä¿¡æ¯æœç´¢",
          description="é€šè¿‡metasoæœç´¢APIè·å–è‚¡ç¥¨ç›¸å…³åŸºæœ¬é¢ä¿¡æ¯ã€æ–°é—»å’Œåˆ†æ")
async def get_fundamental_search(request: FundamentalSearchRequest) -> APIResponse:
    """åŸºæœ¬é¢ä¿¡æ¯æœç´¢"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not fundamental_service.is_configured():
        return APIResponse(ret_code=-1, ret_msg="Metaso API å¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨åŸºæœ¬é¢æœç´¢åŠŸèƒ½", data=None)
    
    try:
        # è°ƒç”¨åŸºæœ¬é¢æœç´¢æœåŠ¡
        result = await fundamental_service.search_fundamental_info(request)
        return APIResponse(
            ret_code=0,
            ret_msg="åŸºæœ¬é¢æœç´¢æˆåŠŸ",
            data=result.dict()
        )
    except Exception as e:
        logger.error(f"åŸºæœ¬é¢æœç´¢å¤±è´¥: {e}")
        error_msg = str(e)
        if "ç½‘ç»œ" in error_msg or "è¿æ¥" in error_msg:
            error_msg = "æœç´¢æœåŠ¡ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        elif "æƒé™" in error_msg or "æˆæƒ" in error_msg:
            error_msg = "æœç´¢æœåŠ¡æƒé™éªŒè¯å¤±è´¥"
        elif "è¶…æ—¶" in error_msg:
            error_msg = "æœç´¢è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        
        return APIResponse(ret_code=-1, ret_msg=f"åŸºæœ¬é¢æœç´¢å¤±è´¥: {error_msg}", data=None)


@app.post("/api/fundamental/stock_search",
          operation_id="get_stock_fundamental",
          summary="ğŸ” è‚¡ç¥¨åŸºæœ¬é¢æœç´¢",
          description="æœç´¢ç‰¹å®šè‚¡ç¥¨çš„åŸºæœ¬é¢ä¿¡æ¯ï¼Œè‡ªåŠ¨æ„å»ºæœç´¢å…³é”®è¯")
async def get_stock_fundamental(request: dict) -> APIResponse:
    """è‚¡ç¥¨åŸºæœ¬é¢æœç´¢ - ç®€åŒ–æ¥å£"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not fundamental_service.is_configured():
        return APIResponse(ret_code=-1, ret_msg="Metaso API å¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨åŸºæœ¬é¢æœç´¢åŠŸèƒ½", data=None)
    
    try:
        stock_code = request.get("stock_code", "")
        stock_name = request.get("stock_name", "")
        
        if not stock_code:
            return APIResponse(ret_code=-1, ret_msg="è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º", data=None)
        
        # è°ƒç”¨è‚¡ç¥¨åŸºæœ¬é¢æœç´¢æœåŠ¡
        return await fundamental_service.search_stock_fundamental(stock_code, stock_name)
        
    except Exception as e:
        logger.error(f"è‚¡ç¥¨åŸºæœ¬é¢æœç´¢å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è‚¡ç¥¨åŸºæœ¬é¢æœç´¢å¤±è´¥: {str(e)}", data=None)


@app.post("/api/fundamental/news/refresh",
          operation_id="refresh_fundamental_news",
          summary="åˆ·æ–°åŸºæœ¬é¢èµ„è®¯",
          description="æ ¹æ®æŒ‡å®šæ•°é‡é‡æ–°æŠ“å– Metaso èµ„è®¯å¹¶æ›´æ–°æœ¬åœ°å­˜å‚¨")
async def refresh_fundamental_news(request: FundamentalNewsRefreshRequest) -> APIResponse:
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not fundamental_service.is_configured():
        return APIResponse(ret_code=-1, ret_msg="Metaso API å¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨åŸºæœ¬é¢æœç´¢åŠŸèƒ½", data=None)
    try:
        signals = await _fetch_news_signals(request.code, request.size, days=request.days)
        return APIResponse(ret_code=0, ret_msg="åˆ·æ–°æˆåŠŸ", data={"signals": signals})
    except Exception as e:
        logger.error(f"åˆ·æ–°åŸºæœ¬é¢èµ„è®¯å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"åˆ·æ–°åŸºæœ¬é¢èµ„è®¯å¤±è´¥: {e}", data=None)


@app.post("/api/fundamental/read_webpage",
          operation_id="read_webpage",
          summary="ğŸ“„ è¯»å–ç½‘é¡µå†…å®¹",
          description="é€šè¿‡metaso reader APIè¯»å–ä»»æ„ç½‘é¡µçš„çº¯æ–‡æœ¬å†…å®¹")
async def read_webpage_endpoint(request: dict) -> APIResponse:
    """è¯»å–ç½‘é¡µå†…å®¹"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not fundamental_service.is_configured():
        return APIResponse(ret_code=-1, ret_msg="Metaso API å¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨ç½‘é¡µè¯»å–åŠŸèƒ½", data=None)
    
    try:
        url = request.get("url", "")
        if not url:
            return APIResponse(ret_code=-1, ret_msg="ç½‘é¡µURLä¸èƒ½ä¸ºç©º", data=None)
        
        # è°ƒç”¨ç½‘é¡µè¯»å–æœåŠ¡
        content = await fundamental_service.read_webpage(url)
        
        return APIResponse(
            ret_code=0,
            ret_msg="ç½‘é¡µè¯»å–æˆåŠŸ",
            data={
                "url": url,
                "content": content,
                "content_length": len(content),
                "api_source": "metaso"
            }
        )
        
    except Exception as e:
        logger.error(f"ç½‘é¡µè¯»å–å¤±è´¥: {e}")
        error_msg = str(e)
        if "ç½‘ç»œ" in error_msg or "è¿æ¥" in error_msg:
            error_msg = "ç½‘é¡µè¯»å–ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        elif "404" in error_msg:
            error_msg = "ç½‘é¡µä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®"
        elif "è¶…æ—¶" in error_msg:
            error_msg = "ç½‘é¡µè¯»å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        
        return APIResponse(ret_code=-1, ret_msg=f"ç½‘é¡µè¯»å–å¤±è´¥: {error_msg}", data=None)


@app.post("/api/fundamental/chat",
          operation_id="chat_completion",
          summary="ğŸ’¬ æ™ºèƒ½é—®ç­”",
          description="é€šè¿‡metaso chat APIè¿›è¡Œæ™ºèƒ½é—®ç­”å¯¹è¯")
async def chat_endpoint(request: dict) -> APIResponse:
    """æ™ºèƒ½é—®ç­”"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    if not fundamental_service.is_configured():
        return APIResponse(ret_code=-1, ret_msg="Metaso API å¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨æ™ºèƒ½é—®ç­”åŠŸèƒ½", data=None)
    
    try:
        messages = request.get("messages", [])
        model = request.get("model", "fast")
        stream = request.get("stream", True)
        
        if not messages:
            return APIResponse(ret_code=-1, ret_msg="å¯¹è¯æ¶ˆæ¯ä¸èƒ½ä¸ºç©º", data=None)
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, dict) and "role" in msg and "content" in msg:
                formatted_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            else:
                formatted_messages.append({
                    "role": "user",
                    "content": str(msg)
                })
        
        # è°ƒç”¨é—®ç­”æœåŠ¡
        answer = await fundamental_service.chat_completion(formatted_messages, model, stream)
        
        return APIResponse(
            ret_code=0,
            ret_msg="é—®ç­”æˆåŠŸ",
            data={
                "answer": answer,
                "model": model,
                "stream": stream,
                "messages": formatted_messages,
                "api_source": "metaso"
            }
        )
        
    except Exception as e:
        logger.error(f"é—®ç­”å¤±è´¥: {e}")
        error_msg = str(e)
        if "ç½‘ç»œ" in error_msg or "è¿æ¥" in error_msg:
            error_msg = "é—®ç­”æœåŠ¡ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        elif "æƒé™" in error_msg or "æˆæƒ" in error_msg:
            error_msg = "é—®ç­”æœåŠ¡æƒé™éªŒè¯å¤±è´¥"
        elif "è¶…æ—¶" in error_msg:
            error_msg = "é—®ç­”è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        
        return APIResponse(ret_code=-1, ret_msg=f"é—®ç­”å¤±è´¥: {error_msg}", data=None)


# ==================== æ³¨æ„äº‹é¡¹ ====================
# æ³¨æ„ï¼šå¯Œé€”APIä¸­æ²¡æœ‰"æŒä»“å†å²"æ¥å£ï¼ŒæŒä»“å†å²éœ€è¦é€šè¿‡å†å²æˆäº¤æ•°æ®è®¡ç®—å¾—å‡º
# å½“å‰æŒä»“åªèƒ½é€šè¿‡ position_list_query è·å–å½“å‰æ—¶ç‚¹çš„æŒä»“ä¿¡æ¯

# ==================== MCPä¸“ç”¨å¢å¼ºæ‹‰å–æ¥å£ ====================

@app.post("/api/quote/realtime_quote_enhanced",
          operation_id="get_realtime_quote_enhanced",
          summary="ğŸš€ MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶æŠ¥ä»·æ‹‰å–",
          description="ä¸“ä¸ºMCPè®¾è®¡çš„å®æ—¶æŠ¥ä»·æ¥å£ï¼Œæ”¯æŒæ‰¹é‡è·å–ï¼Œæ— éœ€è®¢é˜…ï¼Œç›´æ¥æ‹‰å–æœ€æ–°æ•°æ®")
async def get_realtime_quote_enhanced(request: RealtimeQuoteEnhancedRequest) -> APIResponse:
    """MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶æŠ¥ä»·æ‹‰å–"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_realtime_quote_enhanced(request.codes, request.fields)
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå®æ—¶æŠ¥ä»·å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–å¢å¼ºå®æ—¶æŠ¥ä»·å¤±è´¥: {e}", data=None)


@app.post("/api/quote/realtime_orderbook_enhanced",
          operation_id="get_realtime_orderbook_enhanced",
          summary="ğŸš€ MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶æ‘†ç›˜æ‹‰å–",
          description="ä¸“ä¸ºMCPè®¾è®¡çš„å®æ—¶æ‘†ç›˜æ¥å£ï¼Œæ— éœ€è®¢é˜…ï¼Œç›´æ¥æ‹‰å–ä¹°å–ç›˜å£æ•°æ®")
async def get_realtime_orderbook_enhanced(request: RealtimeOrderBookEnhancedRequest) -> APIResponse:
    """MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶æ‘†ç›˜æ‹‰å–"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_realtime_orderbook_enhanced(request.code, request.num)
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå®æ—¶æ‘†ç›˜å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–å¢å¼ºå®æ—¶æ‘†ç›˜å¤±è´¥: {e}", data=None)


@app.post("/api/quote/realtime_ticker_enhanced",
          operation_id="get_realtime_ticker_enhanced",
          summary="ğŸš€ MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶é€ç¬”æ‹‰å–",
          description="ä¸“ä¸ºMCPè®¾è®¡çš„å®æ—¶é€ç¬”æ¥å£ï¼Œæ— éœ€è®¢é˜…ï¼Œç›´æ¥æ‹‰å–æœ€æ–°æˆäº¤æ•°æ®")
async def get_realtime_ticker_enhanced(request: RealtimeTickerEnhancedRequest) -> APIResponse:
    """MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶é€ç¬”æ‹‰å–"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_realtime_ticker_enhanced(request.code, request.num)
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå®æ—¶é€ç¬”å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–å¢å¼ºå®æ—¶é€ç¬”å¤±è´¥: {e}", data=None)


@app.post("/api/quote/realtime_data_enhanced",
          operation_id="get_realtime_data_enhanced",
          summary="ğŸš€ MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶åˆ†æ—¶æ‹‰å–",
          description="ä¸“ä¸ºMCPè®¾è®¡çš„å®æ—¶åˆ†æ—¶æ¥å£ï¼Œæ— éœ€è®¢é˜…ï¼Œç›´æ¥æ‹‰å–åˆ†æ—¶èµ°åŠ¿æ•°æ®")
async def get_realtime_data_enhanced(request: RealtimeDataEnhancedRequest) -> APIResponse:
    """MCPä¸“ç”¨ï¼šå¢å¼ºå®æ—¶åˆ†æ—¶æ‹‰å–"""
    if not _server_ready:
        return APIResponse(ret_code=-1, ret_msg="æœåŠ¡å™¨æ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨åé‡è¯•", data=None)
    
    try:
        return await futu_service.get_realtime_data_enhanced(request.code)
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºå®æ—¶åˆ†æ—¶å¤±è´¥: {e}")
        return APIResponse(ret_code=-1, ret_msg=f"è·å–å¢å¼ºå®æ—¶åˆ†æ—¶å¤±è´¥: {e}", data=None)


# ==================== å¯åŠ¨é…ç½® ====================
# æ³¨é‡Šæ‰åŸæ¥çš„MCPåˆ›å»ºä»£ç ï¼Œç§»åˆ°lifespanä¸­
# mcp = FastApiMCP(
#     app,
#     name="å¯Œé€”è¯åˆ¸å¢å¼ºç‰ˆMCPæœåŠ¡",
#     description="å¢å¼ºç‰ˆå¯Œé€”è¯åˆ¸APIæœåŠ¡ï¼Œé›†æˆ15+æŠ€æœ¯æŒ‡æ ‡ã€æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿã€ä¸“ä¸šé‡åŒ–åˆ†æåŠŸèƒ½ã€‚æ”¯æŒæ¸¯è‚¡ã€ç¾è‚¡ã€Aè‚¡å®æ—¶æŠ¥ä»·ï¼ŒKçº¿æ•°æ®ï¼ŒæŠ€æœ¯åˆ†ææŒ‡æ ‡è®¡ç®—ï¼Œæ™ºèƒ½ç¼“å­˜ä¼˜åŒ–ï¼Œäº¤æ˜“å†å²æŸ¥è¯¢ç­‰åŠŸèƒ½ã€‚æ³¨æ„ï¼šæŒä»“å†å²éœ€é€šè¿‡å†å²æˆäº¤æ•°æ®è®¡ç®—ã€‚"
# )
# 
# # æŒ‚è½½MCPæœåŠ¡åˆ°FastAPIåº”ç”¨
# mcp.mount()

if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨å¯Œé€”MCPå¢å¼ºæœåŠ¡...")
    logger.info("ğŸ“Š Web æ€»è§ˆ: http://localhost:8001/web  (è‹¥è‡ªå®šä¹‰äº† DASHBOARD_BASE_URLï¼Œè¯·æ›¿æ¢æˆå¯¹åº”åŸŸå)")
    
    uvicorn.run(
        "main_enhanced:app",
        host="0.0.0.0",
        port=8001,  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
        reload=False,  # å…³é—­reloadé¿å…åˆå§‹åŒ–é—®é¢˜
        log_level="info"
    )
def _merge_kline_records(left: Optional[List[Dict[str, Any]]], right: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    if not left and not right:
        return []
    if not left:
        return list(right or [])
    if not right:
        return list(left or [])
    merged: Dict[str, Dict[str, Any]] = {}
    for record in left:
        key = record.get("time_key") or record.get("time")
        if not key:
            continue
        merged[key] = dict(record)
    for record in right:
        key = record.get("time_key") or record.get("time")
        if not key:
            continue
        base = merged.get(key, {})
        base.update(record)
        merged[key] = base
    sorted_items = sorted(merged.items(), key=lambda item: item[0])
    return [item[1] for item in sorted_items]


def _build_analysis_context_text(snapshot: Dict[str, Any]) -> str:
    quote = snapshot.get("quote") or {}
    session = snapshot.get("session_window") or {}
    capital_flow = (snapshot.get("capital_flow") or {}).get("summary", {})
    capital_distribution = (snapshot.get("capital_distribution") or {}).get("summary", {})
    insights = snapshot.get("insights") or {}
    generated_at = snapshot.get("generated_at")
    missing_parts: List[str] = []
    lines = []
    if quote:
        lines.append(
            f"[è¡Œæƒ… @ {quote.get('update_time') or generated_at}] ä»·æ ¼ {quote.get('price') or quote.get('cur_price')}ï¼Œ"
            f"æ¶¨è·Œå¹… {quote.get('change_rate')}%ï¼Œæˆäº¤é¢ {quote.get('turnover')}ï¼Œæˆäº¤é‡ {quote.get('volume')}ã€‚"
        )
    holding = snapshot.get("holding") or {}
    if holding:
        parts = [f"{k}:{v}" for k, v in holding.items()]
        lines.append("æŒä»“: " + " / ".join(parts))
    else:
        missing_parts.append("æŒä»“æ˜ç»†")

    if session:
        break_info = ""
        if session.get("break_start") and session.get("break_end"):
            break_info = f"ï¼Œåˆä¼‘ {session['break_start']}~{session['break_end']}"
        lines.append(
            f"äº¤æ˜“æ—¶æ®µ: {session.get('market','æœªçŸ¥å¸‚åœº')} {session.get('open_time')}~{session.get('close_time')}{break_info}"
        )
    else:
        missing_parts.append("äº¤æ˜“æ—¶æ®µ")

    signals = snapshot.get("signals") or {}
    for sentiment in ("bullish", "bearish"):
        items = signals.get(sentiment) or []
        if items:
            highlights = "; ".join(
                f"{(item.get('title') or item.get('summary') or '').strip()} [{item.get('publish_time') or item.get('published_at')}]"
                for item in items[:3]
                if item
            )
            label = "åˆ©å¥½" if sentiment == "bullish" else "åˆ©ç©º"
            lines.append(f"{label}èµ„è®¯: {highlights}")
    if not any(signals.get(key) for key in ("bullish", "bearish", "neutral")):
        missing_parts.append("æœ€æ–°èµ„è®¯/æ–°é—»")

    tech_summary = insights.get("technical_summary")
    if tech_summary:
        lines.append(f"æŠ€æœ¯æ‘˜è¦: {tech_summary}")
    if capital_flow:
        lines.append(
            f"èµ„é‡‘æµ: {capital_flow.get('overall_trend')}ï¼Œä¸»åŠ› {capital_flow.get('main_trend')}ï¼Œ"
            f"æœ€æ–°å‡€æµå…¥ {capital_flow.get('latest_net_inflow')}"
        )
    else:
        missing_parts.append("èµ„é‡‘æµæ‘˜è¦")
    if capital_distribution:
        lines.append(
            f"èµ„é‡‘åˆ†å¸ƒ: ä¸»å¯¼èµ„é‡‘ {capital_distribution.get('dominant_fund_type')} "
            f"{capital_distribution.get('dominant_fund_amount')}ï¼Œæ€»ä½“ {capital_distribution.get('overall_trend')}"
        )
    recommendations = snapshot.get("recommendations") or []
    if recommendations:
        latest = recommendations[0]
        lines.append(
            f"æœ€æ–°ç­–ç•¥: {latest.get('action')} ({latest.get('timeframe')}) "
            f"ä¿¡å¿ƒ {latest.get('confidence')} äº {latest.get('created_at')}"
        )
    quote_counts = [insights.get("signal_bullish"), insights.get("signal_bearish"), insights.get("signal_neutral")]
    if any(v is not None for v in quote_counts):
        lines.append(
            f"ä¿¡å·åˆ†å¸ƒ: åˆ©å¥½ {insights.get('signal_bullish', '-')}"
            f" / åˆ©ç©º {insights.get('signal_bearish', '-')}"
            f" / ä¸­æ€§ {insights.get('signal_neutral', '-')}"
        )
    if not quote:
        missing_parts.append("å®æ—¶è¡Œæƒ…")
    if missing_parts:
        lines.append("âš ï¸ æ•°æ®ç¼ºå£: " + "ã€".join(missing_parts))
    return "\n".join(lines)

class MultiModelAnalysisRequest(BaseModel):
    code: str = Field(..., description="è‚¡ç¥¨ä»£ç , å¦‚ HK.00700")
    models: List[str] = Field(default_factory=lambda: ["deepseek", "kimi"], description="å‚ä¸åˆ†æçš„æ¨¡å‹åˆ—è¡¨")
    judge_model: str = Field("gemini", description="æœ€ç»ˆè¯„å®¡æ¨¡å‹")
    question: Optional[str] = Field(None, description="é¢å¤–å…³æ³¨çš„é—®é¢˜")
